---
title: Few-shot Learning for Text Generation
author: Weizhi Peng
date: 2021-07-02 14:30:00 -0500
categories: [Research Experience]
tags: [LDA, Pytorch, BERT, Transformer, GPT, T5, BART, NLP]
---

# Few-shot Learning for Text Generation

At the prestigious King’s College London, from January 2022 to January 2023, I was engrossed in cutting-edge research centered on Few-shot Learning, a subdomain of machine learning focusing on the capability of models to learn from a limited amount of data. My role as a researcher allowed me to dive into the challenges of text generation—a field where the articulation of coherent and contextually relevant narratives from structured data, like tables, is the prime objective.

## Highlight

- In academic research, pre-training models such as T5, GPT2, Bart, etc are used to generate Table to Text generation by Few-shot Learning and memory is utilized to store pre-training knowledge and training instance selection is used to improve training performance and achieve higher performance.

- Pre-trained models are trained and tested using books, songs and other public datasets, evaluated and optimized using BLEU-4, ROUGE-4 and other methods.
## Related Work

### Employing Advanced Pre-trained Models:

The cornerstone of my research involved leveraging state-of-the-art pre-trained models such as **T5 (Text-to-Text Transfer Transformer)**, **GPT-2 (Generative Pre-trained Transformer 2)**, and **BART (Bidirectional and Auto-Regressive Transformers)**. These models have revolutionized the NLP (Natural Language Processing) landscape with their ability to comprehend and generate human-like text.

### Innovative Approach to Table-to-Text Generation:

My work specifically focused on the novel application of **Few-shot Learning** for generating descriptive text from tabular data. This technique is particularly challenging due to the sparse nature of tabular datasets, where the model must learn to perform tasks with minimal examples. To enhance the efficiency and effectiveness of this learning approach, I explored the use of memory mechanisms that allowed the models to retain and utilize pre-training knowledge, akin to how humans recall information when learning new concepts.

### Training Instance Selection for Performance Boost:

A pivotal aspect of my research was training instance selection. By identifying and utilizing the most informative examples for training, the performance of Few-shot Learning was significantly improved. This methodical selection process was crucial in achieving higher performance with fewer data points, a significant stride in the realm of efficient machine learning.

### Diverse Datasets and Rigorous Evaluation:

The versatility of the models was put to the test using an array of datasets, including literary works like books and creative compositions such as songs, among other public datasets. This not only demonstrated the models' adaptability but also their potential for broader applications in various domains.

For evaluation, I relied on quantitative metrics such as **BLEU-4 (Bilingual Evaluation Understudy)** and **ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation)**, which are established standards in the NLP field for assessing the quality of generated text. Through iterative testing and optimization, the models were fine-tuned to achieve a balance between fluency, relevance, and factual correctness in the generated narratives.

My time at King’s College London was a deep exploration into the synthesis of Few-shot Learning with advanced NLP models, pushing the boundaries of how AI can learn efficiently from limited data to generate meaningful text. The implications of this research are profound, suggesting a future where AI can quickly adapt and generate knowledge across various fields with minimal human intervention.