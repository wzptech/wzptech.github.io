[
  
  {
    "title": "Clustering",
    "url": "/posts/Clustering/",
    "categories": "Deep Learning, Clustering",
    "tags": "Deep Learning, Clustering",
    "date": "2022-04-21 07:32:00 +0800",
    





    "snippet": "Unsupervised LearningClusteringK-meansKmeans python codeimport numpy as npm = [    [2,2],    [5,1]]X = [    [3,4],    [5,1],    [3,7],    [9,6],    [2,2],    [7,0]]result = []while True:    x_m = [(np.array(X) - np.array(center)) for center in m]    distance = [np.sqrt(np.sum(np.power(item,2),axis=1)) for item in x_m]    class_selection = np.argmax(np.transpose(distance),axis=1)    class_num = np.unique(class_selection)    m_prev = m.copy()    for i in range(len(class_num)):        index = np.where(class_selection==class_num[i])[0]        m[i] = np.mean(np.array(X)[index],axis=0)        result = list(np.transpose(np.append(distance,[class_selection],axis=0)).round(4))    # prettytable    # -----------------------------------------------------------    pt = PrettyTable(('sqrt((x-m1)^2)','sqrt((x-m2)^2)','class'))    for row in result: pt.add_row(row)    print(pt)    print('New Center:',m)    if np.array_equal(np.sort(m_prev),np.sort(m)):        print('Meet Converage!')        breakOutput+----------------+----------------+-------+| sqrt((x-m1)^2) | sqrt((x-m2)^2) | class |+----------------+----------------+-------+|     2.2361     |     3.6056     |  1.0  ||     3.1623     |      0.0       |  0.0  ||     5.099      |     6.3246     |  1.0  ||     8.0623     |     6.4031     |  0.0  ||      0.0       |     3.1623     |  1.0  ||     5.3852     |     2.2361     |  0.0  |+----------------+----------------+-------+New Center: [array([7.        , 2.33333333]), array([2.66666667, 4.33333333])]+----------------+----------------+-------+| sqrt((x-m1)^2) | sqrt((x-m2)^2) | class |+----------------+----------------+-------+|     4.3333     |     0.4714     |  0.0  ||     2.4037     |     4.0689     |  1.0  ||     6.1464     |     2.6874     |  0.0  ||     4.1767     |     6.549      |  1.0  ||     5.0111     |     2.4267     |  0.0  ||     2.3333     |     6.1283     |  1.0  |+----------------+----------------+-------+New Center: [array([2.66666667, 4.33333333]), array([7.        , 2.33333333])]Fuzzy K-meansFuzzy K means python codeX = [    [-1,3],    [1,4],    [0,5],    [4,-1],    [3,0],    [5,1]]# disable Scientific notationnp.set_printoptions(suppress=True)# cluster numberK = 2# parameter bb = 2# theta, terminate state, change smaller than tt = 0.5# initial membershipu = [    [1,0],    [0.5,0.5],    [0.5,0.5],    [0.5,0.5],    [0.5,0.5],    [0,1]]# normalise membershipu = [(item/np.sum(item)) for item in u]counter = 0while True:    if counter &gt; 0:        m_prev = m    u_transpose = np.transpose(u)    m = np.dot(np.power(u_transpose,2),X)/np.sum(np.power(u_transpose,2),axis=1)    result = []    for index in range(len(X)):        x = X[index]        x_m = [(np.array(x) - np.array(center)) for center in m]        distance = [np.sqrt(np.sum(np.power(item,2))) for item in x_m]        distance_inverse = np.power([1/item for item in distance],2)        u_update = distance_inverse/sum(distance_inverse)        u[index] = u_update        result.append((index+1,X[index],np.round(m,4),np.round(distance,4),np.round(distance_inverse,4),np.round(u[index],4)))        # prettytable    # -----------------------------------------------------------    pt = PrettyTable(('iteration','Sample X','cluster center m','sqrt(X-m)^2','(1/sqrt(X-m)^2)^2','ui'))    for row in result: pt.add_row(row)    print(pt)    for index in range(len(m)):        print('m'+str(index+1),':',np.round(m[index],4))    if counter &gt; 0:        compare = m_prev - m        if np.all(np.abs(compare)&lt; t):            print('Meet Converage!')            break    counter += 1Output+-----------+----------+------------------+-----------------+-------------------+-----------------+| iteration | Sample X | cluster center m |   sqrt(X-m)^2   | (1/sqrt(X-m)^2)^2 |        ui       |+-----------+----------+------------------+-----------------+-------------------+-----------------+|     1     | [-1, 3]  |    [[0.5 2.5]    | [1.5811 4.7434] |  [0.4    0.0444]  |    [0.9 0.1]    ||           |          |    [3.5 1.5]]    |                 |                   |                 ||     2     |  [1, 4]  |    [[0.5 2.5]    | [1.5811 3.5355] |    [0.4  0.08]    | [0.8333 0.1667] ||           |          |    [3.5 1.5]]    |                 |                   |                 ||     3     |  [0, 5]  |    [[0.5 2.5]    | [2.5495 4.9497] |  [0.1538 0.0408]  | [0.7903 0.2097] ||           |          |    [3.5 1.5]]    |                 |                   |                 ||     4     | [4, -1]  |    [[0.5 2.5]    | [4.9497 2.5495] |  [0.0408 0.1538]  | [0.2097 0.7903] ||           |          |    [3.5 1.5]]    |                 |                   |                 ||     5     |  [3, 0]  |    [[0.5 2.5]    | [3.5355 1.5811] |    [0.08 0.4 ]    | [0.1667 0.8333] ||           |          |    [3.5 1.5]]    |                 |                   |                 ||     6     |  [5, 1]  |    [[0.5 2.5]    | [4.7434 1.5811] |  [0.0444 0.4   ]  |    [0.1 0.9]    ||           |          |    [3.5 1.5]]    |                 |                   |                 |+-----------+----------+------------------+-----------------+-------------------+-----------------+m1 : [0.5 2.5]m2 : [3.5 1.5]+-----------+----------+-------------------+-----------------+-------------------+-----------------+| iteration | Sample X |  cluster center m |   sqrt(X-m)^2   | (1/sqrt(X-m)^2)^2 |        ui       |+-----------+----------+-------------------+-----------------+-------------------+-----------------+|     1     | [-1, 3]  |  [[0.0876 3.7529] | [1.3228 5.6312] |  [0.5715 0.0315]  | [0.9477 0.0523] ||           |          |  [3.9124 0.2471]] |                 |                   |                 ||     2     |  [1, 4]  |  [[0.0876 3.7529] | [0.9453 4.7504] |  [1.1191 0.0443]  | [0.9619 0.0381] ||           |          |  [3.9124 0.2471]] |                 |                   |                 ||     3     |  [0, 5]  |  [[0.0876 3.7529] | [1.2502 6.156 ] |  [0.6398 0.0264]  | [0.9604 0.0396] ||           |          |  [3.9124 0.2471]] |                 |                   |                 ||     4     | [4, -1]  |  [[0.0876 3.7529] | [6.156  1.2502] |  [0.0264 0.6398]  | [0.0396 0.9604] ||           |          |  [3.9124 0.2471]] |                 |                   |                 ||     5     |  [3, 0]  |  [[0.0876 3.7529] | [4.7504 0.9453] |  [0.0443 1.1191]  | [0.0381 0.9619] ||           |          |  [3.9124 0.2471]] |                 |                   |                 ||     6     |  [5, 1]  |  [[0.0876 3.7529] | [5.6312 1.3228] |  [0.0315 0.5715]  | [0.0523 0.9477] ||           |          |  [3.9124 0.2471]] |                 |                   |                 |+-----------+----------+-------------------+-----------------+-------------------+-----------------+m1 : [0.0876 3.7529]m2 : [3.9124 0.2471]+-----------+----------+---------------------+-----------------+-------------------+-----------------+| iteration | Sample X |   cluster center m  |   sqrt(X-m)^2   | (1/sqrt(X-m)^2)^2 |        ui       |+-----------+----------+---------------------+-----------------+-------------------+-----------------+|     1     | [-1, 3]  |  [[ 0.0187  4.0009] | [1.4281 5.8154] |  [0.4903 0.0296]  | [0.9431 0.0569] ||           |          |  [ 3.9813 -0.0009]] |                 |                   |                 ||     2     |  [1, 4]  |  [[ 0.0187  4.0009] | [0.9813 4.9895] |  [1.0385 0.0402]  | [0.9628 0.0372] ||           |          |  [ 3.9813 -0.0009]] |                 |                   |                 ||     3     |  [0, 5]  |  [[ 0.0187  4.0009] | [0.9993 6.3921] |  [1.0014 0.0245]  | [0.9761 0.0239] ||           |          |  [ 3.9813 -0.0009]] |                 |                   |                 ||     4     | [4, -1]  |  [[ 0.0187  4.0009] | [6.3921 0.9993] |  [0.0245 1.0014]  | [0.0239 0.9761] ||           |          |  [ 3.9813 -0.0009]] |                 |                   |                 ||     5     |  [3, 0]  |  [[ 0.0187  4.0009] | [4.9895 0.9813] |  [0.0402 1.0385]  | [0.0372 0.9628] ||           |          |  [ 3.9813 -0.0009]] |                 |                   |                 ||     6     |  [5, 1]  |  [[ 0.0187  4.0009] | [5.8154 1.4281] |  [0.0296 0.4903]  | [0.0569 0.9431] ||           |          |  [ 3.9813 -0.0009]] |                 |                   |                 |+-----------+----------+---------------------+-----------------+-------------------+-----------------+m1 : [0.0187 4.0009]m2 : [ 3.9813 -0.0009]Meet Converage!Iterative OptimizationHierarchical Clusteringhierarchical agglomerative python codeimport numpy as npfrom prettytable import PrettyTableX = [    [-1,3],    [1,2],    [0,1],    [4,0],    [5,4],    [3,2]]# terminate state, cluster = 3c = 3def get_mean(input,y=0):    if len(np.shape(input))&gt;1:        return np.mean(input,axis=0)    else:        return inputdef get_mean_distance(x,y):    m = get_mean(x)    n = get_mean(y)    return np.sqrt(np.sum(np.power(np.array(m)-np.array(n),2)))def get_avg_distance(x,y):    distance = []    if len(np.shape(x))&gt;1 and len(np.shape(y))&gt;1:        for m in x:            for n in y:                distance.append(np.sqrt(np.sum(np.power(np.array(m)-np.array(n),2))))        return np.mean(distance)    elif len(np.shape(x))&gt;1 and len(np.shape(y))&lt;=1:        for m in x:            distance.append(np.sqrt(np.sum(np.power(np.array(m)-np.array(y),2))))        return np.mean(distance)    elif len(np.shape(x))&lt;=1 and len(np.shape(y))&gt;1:        for m in y:            distance.append(np.sqrt(np.sum(np.power(np.array(m)-np.array(x),2))))        return np.mean(distance)    else:        return np.sqrt(np.sum(np.power(np.array(x)-np.array(y),2)))def get_max_distance(x,y):    best = 0    if len(np.shape(x))&gt;1 and len(np.shape(y))&gt;1:        for m in x:            for n in y:                distance = np.sqrt(np.sum(np.power(np.array(m)-np.array(n),2)))                if distance &gt; best:                    best = distance        return best    elif len(np.shape(x))&gt;1 and len(np.shape(y))&lt;=1:        for m in x:            distance = np.sqrt(np.sum(np.power(np.array(m)-np.array(y),2)))            if distance &gt; best:                best = distance        return best    elif len(np.shape(x))&lt;=1 and len(np.shape(y))&gt;1:        for m in y:            distance = np.sqrt(np.sum(np.power(np.array(m)-np.array(x),2)))            if distance &gt; best:                best = distance        return best    else:        return np.sqrt(np.sum(np.power(np.array(x)-np.array(y),2)))def get_min_distance(x,y):    best = 9999    if len(np.shape(x))&gt;1 and len(np.shape(y))&gt;1:        for m in x:            for n in y:                distance = np.sqrt(np.sum(np.power(np.array(m)-np.array(n),2)))                if distance &lt; best:                    best = distance        return best    elif len(np.shape(x))&gt;1 and len(np.shape(y))&lt;=1:        for m in x:            distance = np.sqrt(np.sum(np.power(np.array(m)-np.array(y),2)))            if distance &lt; best:                best = distance        return best    elif len(np.shape(x))&lt;=1 and len(np.shape(y))&gt;1:        for m in y:            distance = np.sqrt(np.sum(np.power(np.array(m)-np.array(x),2)))            if distance &lt; best:                best = distance        return best    else:        return np.sqrt(np.sum(np.power(np.array(x)-np.array(y),2)))def add_list(x,y):    if len(np.shape(x))&gt;1 and len(np.shape(y))&gt;1:        return x+y    elif len(np.shape(x))&lt;=1 and len(np.shape(y))&gt;1:        return y+[x]    elif len(np.shape(x))&gt;1 and len(np.shape(y))&lt;=1:        return x+[y]    else:        return [x,y]X_new = Xfor index in range(len(X_new)):        print('Cluster',index+1,':',X_new[index])while True:    best = 10000    result = []    for x_index in range(len(X_new)-1):        x = X_new[x_index]        y = X_new[x_index+1:]        distance = [get_min_distance(x,m) for m in y]        result.append([x_index+1] + list(np.append(np.ones(x_index+1)*0,np.round(distance,4))))        if best&gt;np.min(distance):            best_list = add_list(X_new[x_index],X_new[x_index + np.argmin(distance) + 1])            best_index = [x_index,x_index + np.argmin(distance) + 1]            best = np.min(distance)    # prettytable    # -----------------------------------------------------------    title = ['xi']+[str(i) for i in list(range(1,len(X_new)+1))]    pt = PrettyTable(title)    for row in result: pt.add_row(row)    print(pt)    # update X    X_new = np.delete(X_new,best_index,axis=0)    X_new = [best_list] + X_new.tolist()    for index in range(len(X_new)):        print('Cluster',index+1,':',X_new[index])    if len(X_new) &lt;=3 :        breakOutputCluster 1 : [-1, 3]Cluster 2 : [1, 2]Cluster 3 : [0, 1]Cluster 4 : [4, 0]Cluster 5 : [5, 4]Cluster 6 : [3, 2]+----+-----+--------+--------+--------+--------+--------+| xi |  1  |   2    |   3    |   4    |   5    |   6    |+----+-----+--------+--------+--------+--------+--------+| 1  | 0.0 | 2.2361 | 2.2361 | 5.831  | 6.0828 | 4.1231 || 2  | 0.0 |  0.0   | 1.4142 | 3.6056 | 4.4721 |  2.0   || 3  | 0.0 |  0.0   |  0.0   | 4.1231 | 5.831  | 3.1623 || 4  | 0.0 |  0.0   |  0.0   |  0.0   | 4.1231 | 2.2361 || 5  | 0.0 |  0.0   |  0.0   |  0.0   |  0.0   | 2.8284 |+----+-----+--------+--------+--------+--------+--------+Cluster 1 : [[1, 2], [0, 1]]Cluster 2 : [-1, 3]Cluster 3 : [4, 0]Cluster 4 : [5, 4]Cluster 5 : [3, 2]+----+-----+--------+--------+--------+--------+| xi |  1  |   2    |   3    |   4    |   5    |+----+-----+--------+--------+--------+--------+| 1  | 0.0 | 2.2361 | 3.8643 | 5.1515 | 2.5811 || 2  | 0.0 |  0.0   | 5.831  | 6.0828 | 4.1231 || 3  | 0.0 |  0.0   |  0.0   | 4.1231 | 2.2361 || 4  | 0.0 |  0.0   |  0.0   |  0.0   | 2.8284 |+----+-----+--------+--------+--------+--------+Cluster 1 : [[1, 2], [0, 1], [-1, 3]]Cluster 2 : [4, 0]Cluster 3 : [5, 4]Cluster 4 : [3, 2]+----+-----+--------+--------+--------+| xi |  1  |   2    |   3    |   4    |+----+-----+--------+--------+--------+| 1  | 0.0 | 4.5199 | 5.462  | 3.0951 || 2  | 0.0 |  0.0   | 4.1231 | 2.2361 || 3  | 0.0 |  0.0   |  0.0   | 2.8284 |+----+-----+--------+--------+--------+Cluster 1 : [[4, 0], [3, 2]]Cluster 2 : [[1, 2], [0, 1], [-1, 3]]Cluster 3 : [5, 4]Competitive LearningCompetitive learning alogrithm without normalization python codem = [    [-0.5,1.5],    [0,2.5],    [1.5,0]]X = [    [-1,3],    [1,4],    [0,5],    [4,-1],    [3,0],    [5,1]]# learning raten = 0.1sample = [    [0,5],    [-1,3],    [-1,3],    [3,0],    [5,1]]result = []for index in range(len(sample)):    x = sample[index]    x_m = [(np.array(x) - np.array(center)) for center in m]    distance = [np.sqrt(np.sum(np.power(item,2))) for item in x_m]    j = np.argmin(distance)    m[j] = np.array(m[j]) + n*(x - np.array(m[j]))    result.append((index+1,sample[index],np.round(distance,4),j+1,m[j]))# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration','Sample X','sqrt(X-m)^2','j = argmin(x-mj)','mj'))for row in result: pt.add_row(row)print(pt)for index in range(len(m)):    print('m'+str(index+1),':',m[index])Output+-----------+----------+------------------------+------------------+-------------------+| iteration | Sample X |      sqrt(X-m)^2       | j = argmin(x-mj) |         mj        |+-----------+----------+------------------------+------------------+-------------------+|     1     |  [0, 5]  | [3.5355 2.5    5.2202] |        2         |    [0.   2.75]    ||     2     | [-1, 3]  | [1.5811 1.0308 3.9051] |        2         |  [-0.1    2.775]  ||     3     | [-1, 3]  | [1.5811 0.9277 3.9051] |        2         | [-0.19    2.7975] ||     4     |  [3, 0]  | [3.8079 4.2429 1.5   ] |        3         |    [1.65 0.  ]    ||     5     |  [5, 1]  | [5.5227 5.4925 3.4961] |        3         |   [1.985 0.1  ]   |+-----------+----------+------------------------+------------------+-------------------+m1 : [-0.5, 1.5]m2 : [-0.19    2.7975]m3 : [1.985 0.1  ]Clustering for Unknown Number of ClustersBasic leader follower algorithm python codeX = [    [-1,3],    [1,4],    [0,5],    [4,-1],    [3,0],    [5,1]]# thetao = 3# learning raten = 0.5sample = [    [0,5],    [-1,3],    [-1,3],    [3,0],    [5,1]]m = []result = []for index in range(len(sample)):    x = sample[index]    if len(m) == 0:        m.append(x)    m_prev = m.copy()    x_m = [(np.array(x) - np.array(center)) for center in m]    distance = [np.sqrt(np.sum(np.power(item,2))) for item in x_m]    j = np.argmin(distance)    if distance[j] &lt; o:        m[j] = np.array(m[j]) + n*(x - np.array(m[j]))    else:        m.append(x)        result.append((index+1,sample[index],np.array(m_prev).round(4),np.round(distance,4),j+1,(distance[j]&lt;o),np.array(m).round(4)))# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration','Sample X','cluster center m','sqrt(X-m)^2','j = argmin(x-mj)','|x-m| &lt; theta','mj = mj+n(x-mj)'))for row in result: pt.add_row(row)print(pt)for index in range(len(m)):    print('m'+str(index+1),':',m[index])Output+-----------+----------+------------------+-----------------+------------------+---------------+-----------------+| iteration | Sample X | cluster center m |   sqrt(X-m)^2   | j = argmin(x-mj) | |x-m| &lt; theta | mj = mj+n(x-mj) |+-----------+----------+------------------+-----------------+------------------+---------------+-----------------+|     1     |  [0, 5]  |     [[0 5]]      |       [0.]      |        1         |      True     |    [[0. 5.]]    ||     2     | [-1, 3]  |    [[0. 5.]]     |     [2.2361]    |        1         |      True     |  [[-0.5  4. ]]  ||     3     | [-1, 3]  |  [[-0.5  4. ]]   |     [1.118]     |        1         |      True     | [[-0.75  3.5 ]] ||     4     |  [3, 0]  | [[-0.75  3.5 ]]  |     [5.1296]    |        1         |     False     |  [[-0.75  3.5 ] ||           |          |                  |                 |                  |               |  [ 3.    0.  ]] ||     5     |  [5, 1]  |  [[-0.75  3.5 ]  | [6.27   2.2361] |        2         |      True     |  [[-0.75  3.5 ] ||           |          |  [ 3.    0.  ]]  |                 |                  |               |  [ 4.    0.5 ]] |+-----------+----------+------------------+-----------------+------------------+---------------+-----------------+m1 : [-0.75  3.5 ]m2 : [4.  0.5]"
  },
  
  {
    "title": "Ensemble Methods",
    "url": "/posts/Ensemble-Methods/",
    "categories": "Deep Learning, Ensemble Methods",
    "tags": "Deep Learning, Ensemble Methods",
    "date": "2022-04-11 07:32:00 +0800",
    





    "snippet": "Bagging and BoostingEnsemble learningAdaptive BoostAdaboost - Adaptive Boost Training python codeimport numpy as npfrom prettytable import PrettyTableclass_1 = [1,2,3,4,5]class_2 = [6,7,8,9,10]h_error = [    [3,4,5],    [6,7,8],    [1,2,9]]# Adaboost - Adaptive Boost Training# -----------------------------------------------------------item_number = len(class_1) + len(class_2)w = np.ones(item_number)/item_numbera_list = []for h_index in range(len(h_error)):    result = []    w_prev = w    error_rate = [sum([w[index - 1] for index in sample]) for sample in h_error]    error_min = np.min(error_rate)    h_argmax = np.argmin(error_rate)    # a = 1/2*ln((1-err_min)/err_min)    a = 0.5*np.log((1-error_min)/error_min)    update_list = []    for index in range(item_number):        if (index+1) in h_error[h_index]:            # w(i)*e^(-a*y*h(x))            update_list.append(w[index]*np.exp(a))        else:            # w(i)*e^(-a*y*h(x))            update_list.append(w[index]*np.exp(-a))    z = sum(update_list)    # w_new(i) = w(i)*e^(-a*y*h(x))/z    w = update_list/z        for index in range(item_number):        result.append([index+1,np.array(error_rate).round(4),round(error_min,4),round(a,4),round(w_prev[index],4),round(update_list[index],4),round(w[index],4)])    # prettytable    # -----------------------------------------------------------    pt = PrettyTable(('i','error rate','error_avgmin','a = 1/2*ln((1-err_min)/err_min)','w(i)', 'w(i)*e^(-a*y*h(x))','w_new(i) = w(i)*e^(-a*y*h(x))/z'))    for row in result: pt.add_row(row)    print(pt)    a_list.append(str(round(a,4))+'h'+str(h_argmax+1))result = ''for index in range(len(a_list)):    if index == 0:        result = result + a_list[index]    else:        result = result +' + '+ a_list[index]print('Final classifier: sgn(',result,')')Output+----+---------------+--------------+---------------------------------+------+--------------------+---------------------------------+| i  |   error rate  | error_avgmin | a = 1/2*ln((1-err_min)/err_min) | w(i) | w(i)*e^(-a*y*h(x)) | w_new(i) = w(i)*e^(-a*y*h(x))/z |+----+---------------+--------------+---------------------------------+------+--------------------+---------------------------------+| 1  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.0655       |              0.0714             || 2  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.0655       |              0.0714             || 3  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.1528       |              0.1667             || 4  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.1528       |              0.1667             || 5  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.1528       |              0.1667             || 6  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.0655       |              0.0714             || 7  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.0655       |              0.0714             || 8  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.0655       |              0.0714             || 9  | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.0655       |              0.0714             || 10 | [0.3 0.3 0.3] |     0.3      |              0.4236             | 0.1  |       0.0655       |              0.0714             |+----+---------------+--------------+---------------------------------+------+--------------------+---------------------------------++----+------------------------+--------------+---------------------------------+--------+--------------------+---------------------------------+| i  |       error rate       | error_avgmin | a = 1/2*ln((1-err_min)/err_min) |  w(i)  | w(i)*e^(-a*y*h(x)) | w_new(i) = w(i)*e^(-a*y*h(x))/z |+----+------------------------+--------------+---------------------------------+--------+--------------------+---------------------------------+| 1  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.0714 |       0.0373       |              0.0455             || 2  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.0714 |       0.0373       |              0.0455             || 3  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.1667 |       0.087        |              0.1061             || 4  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.1667 |       0.087        |              0.1061             || 5  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.1667 |       0.087        |              0.1061             || 6  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.0714 |       0.1368       |              0.1667             || 7  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.0714 |       0.1368       |              0.1667             || 8  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.0714 |       0.1368       |              0.1667             || 9  | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.0714 |       0.0373       |              0.0455             || 10 | [0.5    0.2143 0.2143] |    0.2143    |              0.6496             | 0.0714 |       0.0373       |              0.0455             |+----+------------------------+--------------+---------------------------------+--------+--------------------+---------------------------------++----+------------------------+--------------+---------------------------------+--------+--------------------+---------------------------------+| i  |       error rate       | error_avgmin | a = 1/2*ln((1-err_min)/err_min) |  w(i)  | w(i)*e^(-a*y*h(x)) | w_new(i) = w(i)*e^(-a*y*h(x))/z |+----+------------------------+--------------+---------------------------------+--------+--------------------+---------------------------------+| 1  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.0455 |       0.1144       |              0.1667             || 2  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.0455 |       0.1144       |              0.1667             || 3  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.1061 |       0.0421       |              0.0614             || 4  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.1061 |       0.0421       |              0.0614             || 5  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.1061 |       0.0421       |              0.0614             || 6  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.1667 |       0.0662       |              0.0965             || 7  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.1667 |       0.0662       |              0.0965             || 8  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.1667 |       0.0662       |              0.0965             || 9  | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.0455 |       0.1144       |              0.1667             || 10 | [0.3182 0.5    0.1364] |    0.1364    |              0.9229             | 0.0455 |       0.0181       |              0.0263             |+----+------------------------+--------------+---------------------------------+--------+--------------------+---------------------------------+Final classifier: sgn( 0.4236h1 + 0.6496h2 + 0.9229h3 )Stacked generalizationStructure of ensemble classiﬁersDecision TreesEntropy python codedef entropy_impurity(p):    inp = 0    for i in p:        if i == 0:            inp += 0        else:            inp += - i * np.log2(i)    return inpdef drop_impurity(p, p2):    allp = entropy_impurity(p)    sump = 0    for i in range(len(p2)):        sump += -(p[i] * entropy_impurity(p2[i]))    allp += sump    return allpp2 = np.array([[0 / 6, 6 / 6], [2 / 8, 6 / 8]])data = np.array([[8 / 14], [6 / 14]])print(entropy_impurity(data))print(drop_impurity(data, p2))Output[0.98522814][0.63753751]Bagging Decision TreesRandom ForestsFast (scalable), Accurate, Simple to implement, Popular"
  },
  
  {
    "title": "Support Vector Machines",
    "url": "/posts/Support-Vector-Machines/",
    "categories": "Deep Learning, Support Vector Machines",
    "tags": "Deep Learning, Support Vector Machines",
    "date": "2022-03-31 07:32:00 +0800",
    





    "snippet": "Nonlinear SVMsMulti-class SVMsSVM find hyperplane by support vectors python codedef find_weight(support_vector_l, label_l):    # First get the final result matrix from support vector labels    # and sum of all lambda*lable(should equal to 0)    lambdas_result_list = []    for i in label_l:        lambdas_result_list.append(i)    lambdas_result_list.append(0)    # Set a list to store data infront each lambda    # e.g. 2(λ1) + 3(λ2) then we store 2 and 3 into the list    # The data infront each lambda is the coordinate of support vectors    lambdas = []    lambdas_weight0 = []    for i in range(len(label_l)):        lambdas.append(label_l[i] * support_vector_l[i])    lambdas = np.array(lambdas)    # Store value infront lambda after calculate    # yi((w^T)*x + w0) = 1    for i in range(len(support_vector_l)):        temp_list = []        for data in lambdas:            temp_list.append(np.dot(data, support_vector_l[i]))        temp_list.append(1)        lambdas_weight0.append(temp_list)    # Store value infront lambdas from function:    # sum all λiyi = 0    # In this function only support vectors lambda is non-zero    sum_of_lambdas = []    for i in label_l:        sum_of_lambdas.append(i)    sum_of_lambdas.append(0)    # Store the result from function mentioned above    # prepare for matrix calculation    lambdas_weight0.append(sum_of_lambdas)    # Get the result of all unknown lambdas and weight0 from matrix operation    # The calculation is like    # [1, 2, 3, 4]    [λ1]   [1]    # [5, 6, 7 ,8]  * [λ2] = [1]    # [1, 3, 4, 6]    [λ3]   [-1]    # [8, 6, 4, 2]    [λ4]   [0]    # calculated by invert the left hand matrix and dot product it with the right hand matrix    invers_matrix = np.linalg.inv(lambdas_weight0)    lambda_weight0_result = np.dot(invers_matrix, lambdas_result_list)    lambdas_result = lambda_weight0_result[:len(support_vector_l)]    print(lambdas_result)    weight0 = lambda_weight0_result[-1]    # from all data we known get the w    # where w = x1λ1 + x2λ2 +.......    temp = []    for index in range(len(lambdas)):        temp.append(np.dot(lambdas[index], lambdas_result[index]))    weight = np.sum(temp, axis=0, dtype=np.float32)    return weight, weight0support_v = np.array([[3,1], [3,-1], [1,0]])label = [1, 1, -1]w, w0 = find_weight(support_v, label)print(w)print(w0)Output[0.25 0.25 0.5 ][1. 0.]-2.0"
  },
  
  {
    "title": "Feature Extraction",
    "url": "/posts/Feature-Extraction/",
    "categories": "Deep Learning, Feature Extraction",
    "tags": "Deep Learning, Feature Extraction",
    "date": "2022-03-21 07:32:00 +0800",
    





    "snippet": "Principal Components AnalysisTraditional PCA python codeimport numpy as npdef pca(dataset, dim):    X = dataset    print(X.mean(axis=1, keepdims=True))    Y = X - X.mean(axis=1, keepdims=True)    print(\"X-mean = \\n\",Y)    C = []    for i in range(len(np.transpose(Y))):        C.append(np.dot(Y[:, [i]], np.transpose(Y[:, [i]])))    C = np.array(C)    C = np.sum(C, axis=0) / len(np.transpose(Y))    print(\"Conv = \\n\", C)    egv, egva = np.linalg.eigh(C)    print(\"egv = \\n\", egv)    print(\"egvalue =  \\n\", egva)    vnew = []    for i in range(dim):        result = np.where(egv == np.partition(egv.flatten(), -(i + 1))[-(i + 1)])        vnew.append(egva[:, result[0][0]])    vnew = np.array(vnew)    print(vnew)    newsample = np.dot(vnew, Y)    return newsampledata = np.array([[4, 2, 2], [0, -2, 2], [2, 4, 2], [-2, 0, 2]])data = np.transpose(data)print(\"result = \\n\", pca(data, 2))Output[[1.][1.][2.]]X-mean = [[ 3. -1.  1. -3.][ 1. -3.  3. -1.][ 0.  0.  0.  0.]]Conv = [[5. 3. 0.][3. 5. 0.][0. 0. 0.]]egv = [0. 2. 8.]egvalue =  [[ 0.         -0.70710678  0.70710678][ 0.          0.70710678  0.70710678][ 1.          0.          0.        ]][[ 0.70710678  0.70710678  0.        ][-0.70710678  0.70710678  0.        ]]result = [[ 2.82842712 -2.82842712  2.82842712 -2.82842712][-1.41421356 -1.41421356  1.41421356  1.41421356]]Neural Networks for PCA (Hebbian learning) python codeimport numpy as npfrom prettytable import PrettyTable# configuration variables# -----------------------------------------------------------# initial valuesw = [0.5,-0.2]# learning raten = 0.1# iterations# iterations = 150epoch = 2# dataset# -----------------------------------------------------------X = [[0,1],[1,2],[3,1],[-1,-2],[-3,-2]]# Neural Networks for PCA algorithm# -----------------------------------------------------------result = []for o in range(epoch):    for i in range(len(X)):        w_prev = w        x = X[i]        y = np.dot(x,np.transpose(w))        # calculate update part, ηyx        update = (n*y) * np.array(x)        # add update part to w        w = np.add(w, update)        # append result        result.append((str(i + 1 + (len(X) * o)), np.round(w_prev, 4), np.round(x, 4), np.round(y, 4), np.round(update, 4),np.round(w, 4)))# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration', 'w', 'x', 'y', 'ηyx','w_new'))for row in result: pt.add_row(row)print(pt)Output+-----------+-----------------+---------+----------+-----------------+-------------------+| iteration |        w        |    x    |    y     |       ηyx       |       w_new       |+-----------+-----------------+---------+----------+-----------------+-------------------+|     1     |   [ 0.5 -0.2]   |  [0 1]  |   -0.2   |  [-0.   -0.02]  |   [ 0.5  -0.22]   ||     2     |  [ 0.5  -0.22]  |  [1 2]  |   0.06   |  [0.006 0.012]  |  [ 0.506 -0.208]  ||     3     | [ 0.506 -0.208] |  [3 1]  |   1.31   |  [0.393 0.131]  |  [ 0.899 -0.077]  ||     4     | [ 0.899 -0.077] | [-1 -2] |  -0.745  | [0.0745 0.149 ] |  [0.9735 0.072 ]  ||     5     | [0.9735 0.072 ] | [-3 -2] | -3.0645  | [0.9194 0.6129] |  [1.8928 0.6849]  ||     6     | [1.8928 0.6849] |  [0 1]  |  0.6849  | [0.     0.0685] |  [1.8928 0.7534]  ||     7     | [1.8928 0.7534] |  [1 2]  |  3.3996  | [0.34   0.6799] |  [2.2328 1.4333]  ||     8     | [2.2328 1.4333] |  [3 1]  |  8.1318  | [2.4395 0.8132] |  [4.6723 2.2465]  ||     9     | [4.6723 2.2465] | [-1 -2] | -9.1653  | [0.9165 1.8331] |  [5.5889 4.0796]  ||     10    | [5.5889 4.0796] | [-3 -2] | -24.9257 | [7.4777 4.9851] | [13.0666  9.0647] |+-----------+-----------------+---------+----------+-----------------+-------------------+Neural Networks for PCA (Oja’s rule)import numpy as npfrom prettytable import PrettyTable# configuration variables# -----------------------------------------------------------# initial valuesw = [0.5,-0.2]# learning raten = 0.1# iterations# iterations = 150epoch = 2# dataset# -----------------------------------------------------------X = [[0,1],[1,2],[3,1],[-1,-2],[-3,-2]]# Neural Networks for PCA algorithm# -----------------------------------------------------------result = []for o in range(epoch):    for i in range(len(X)):        w_prev = w        x = X[i]        y = np.dot(x,np.transpose(w))        # calculate update part, ηyx        update = (n*y) * (np.array(x) - np.dot(y,w))        # add update part to w        w = np.add(w, update)        # append result        result.append((str(i + 1 + (len(X) * o)), np.round(w_prev, 4), np.round(x, 4), np.round(y, 4), np.round(update, 4),np.round(w, 4)))# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration', 'w', 'x', 'y', 'ηyx','w_new'))for row in result: pt.add_row(row)print(pt)Output+-----------+-------------------+---------+---------+-------------------+-------------------+| iteration |         w         |    x    | y       |        ηyx        |       w_new       |+-----------+-------------------+---------+---------+-------------------+-------------------+|     1     |    [ 0.5 -0.2]    |  [0 1]  | -0.2    | [-0.002  -0.0192] | [ 0.498  -0.2192] ||     2     | [ 0.498  -0.2192] |  [1 2]  | 0.0596  |  [0.0058 0.012 ]  | [ 0.5038 -0.2072] ||     3     | [ 0.5038 -0.2072] |  [3 1]  | 1.3041  |  [0.3056 0.1657]  | [ 0.8093 -0.0415] ||     4     | [ 0.8093 -0.0415] | [-1 -2] | -0.7263 |  [0.0299 0.1474]  |  [0.8393 0.1059]  ||     5     |  [0.8393 0.1059]  | [-3 -2] | -2.7296 |  [0.1936 0.467 ]  |  [1.0328 0.5729]  ||     6     |  [1.0328 0.5729]  |  [0 1]  | 0.5729  | [-0.0339  0.0385] |  [0.9989 0.6114]  ||     7     |  [0.9989 0.6114]  |  [1 2]  | 2.2217  | [-0.2709  0.1425] |   [0.728 0.754]   ||     8     |   [0.728 0.754]   |  [3 1]  | 2.938   |  [ 0.253 -0.357]  |  [0.981  0.3969]  ||     9     |  [0.981  0.3969]  | [-1 -2] | -1.7749 | [-0.1316  0.2299] |  [0.8495 0.6269]  ||     10    |  [0.8495 0.6269]  | [-3 -2] | -3.8021 | [-0.0873 -0.1458] |  [0.7621 0.4811]  |+-----------+-------------------+---------+---------+-------------------+-------------------+Whitening TransformLinear Discriminant AnalysisLinear Discriminant Analysis (LDA) python codew1 = [-1,5]w2 = [2,-3]x1 = [    [1,2],    [2,1],    [3,3]]x2 = [    [6,5],    [7,8]]result = []def J(w,x1,x2):    sb = np.power(np.abs(np.dot(w,np.transpose(np.array(x1).mean(axis=0)-np.array(x2).mean(axis=0)))),2)        sw = sum(np.power(np.dot(w,np.transpose(x1 - np.array(x1).mean(axis=0))),2)) + sum(np.power(np.dot(w,np.transpose(x2 - np.array(x2).mean(axis=0))),2))    global result    result.append([w,np.array(x1).mean(axis=0),np.array(x2).mean(axis=0),np.array(x1).mean(axis=0)-np.array(x2).mean(axis=0),sb,sw,round(sb/sw,4)])    return sb/swJ(w1,x1,x2)J(w2,x1,x2)pt = PrettyTable(('w','m1','m2','m1-m2','sb =  |wT (m1 - m2)|^2','sw = (sum(wT(x-m1))^2 + sum(wT(x-m2))^2','J(w) = sb/sw'))for row in result: pt.add_row(row)print(pt)Output+---------+---------+-----------+-------------+------------------------+----------------------------------------+--------------+|    w    |    m1   |     m2    |    m1-m2    | sb =  |wT (m1 - m2)|^2 | sw = (sum(wT(x-m1))2 + sum(wT(x-m2))^2 | J(w) = sb/sw |+---------+---------+-----------+-------------+------------------------+----------------------------------------+--------------+| [-1, 5] | [2. 2.] | [6.5 6.5] | [-4.5 -4.5] |         324.0          |                 140.0                  |    2.3143    || [2, -3] | [2. 2.] | [6.5 6.5] | [-4.5 -4.5] |         20.25          |                  38.5                  |    0.526     |+---------+---------+-----------+-------------+------------------------+----------------------------------------+--------------+Independent Component Analysis (ICA)Random ProjectionsSparse CodingSparse Coding python codey2 = [0,0,1,0,0,0,-1,0]V = [[0.4 ,0.55, 0.5, -0.1, -0.5, 0.9, 0.5, 0.45],[-0.6, -0.45, -0.5, 0.9, -0.5, 0.1, 0.5, 0.55]]x = [-0.05,-0.95]def error(x,V,y):    # x - Vy    temp = x - np.dot(V,np.transpose(y))    # educian    return np.sqrt(sum(np.power(temp,2)))    print(round(error(x,V,y2),4))Output0.0707"
  },
  
  {
    "title": "Deep Generative Neural Networks",
    "url": "/posts/Deep-Generative-Neural-Networks/",
    "categories": "Deep Learning, Deep Generative Neural Networks",
    "tags": "Deep Learning, Deep Generative Neural Networks",
    "date": "2022-03-11 07:32:00 +0800",
    





    "snippet": "Maximum LikelihoodGenerate image**Generative Adversarial Networks**Generative Adversarial Networks python code# calculating cost functionimport numpy as npX = [    [1,2],    [3,4]]X = np.array(X)X_pred = [    [5,6],    [7,8]]X_pred = np.array(X_pred)weight = np.ones(2)*0.5# define of Discriminatordef Dx(x,a = 0.1,b = 0.2):    x = np.transpose(x)    return 1/(1+np.exp(-a*x[0]+b*x[1]+2))# Calculate the costdef V(x,x_pred = X_pred ,weight = weight,Dx = Dx):    # E(ln(D(x)))    E_lnDx = np.dot(np.log(Dx(X)),weight.transpose())    # E(ln(1-D(G(x)))),G(x) = X_pred    E_ln_1_DGx = np.dot(np.log(1-Dx(X_pred)),weight.transpose())    return E_lnDx + E_ln_1_DGxprint(V(X,X_pred,weight,Dx))Output-2.5465207684425333Continue Python codefrom jax import gradimport jax.numpy as jnpimport numpy as npa = [0.1,0.2]def Dm_1(a,x = X, b = 0.2):    temp = 1/(jnp.exp(jnp.dot(jnp.array([-a,b]),jnp.transpose(x)) + 2)+1)    return temp    def Dm_2(a,x = X[0], b = 0.1):    return 1/(jnp.exp(jnp.dot(jnp.array([-b,a]),jnp.transpose(x)) + 2)+1)def multi_Dm_1(a,x):    result = []    for sample in x:        temp_func = grad(Dm_1)(a,sample)        result.append(temp_func)    return np.array(result)def multi_Dm_2(a,x):    result = []    for sample in x:        temp_func = grad(Dm_2)(a,sample)        result.append(temp_func)    return np.array(result)print(1/Dx(X)*multi_Dm_1(0.1,X))Output[0.90887703 2.77242559]Continue Python code# Calculate gradient updatefrom jax import gradimport jax.numpy as jnpimport numpy as npX = [    [1,2],    [3,4]]X = np.array(X)X_pred = [    [5,6],    [7,8]]X_pred = np.array(X_pred)weight = np.ones(2)*0.5# define learning raten = 0.02a = [0.1,0.2]def Dm_1(a,x = X, b = 0.2):    temp = 1/(jnp.exp(jnp.dot(jnp.array([-a,b]),jnp.transpose(x)) + 2)+1)    return temp    def Dm_2(a,x = X[0], b = 0.1):    return 1/(jnp.exp(jnp.dot(jnp.array([-b,a]),jnp.transpose(x)) + 2)+1)def multi_Dm_1(a,x):    result = []    for sample in x:        temp_func = grad(Dm_1)(a,sample)        result.append(temp_func)    return np.array(result)def multi_Dm_2(a,x):    result = []    for sample in x:        temp_func = grad(Dm_2)(a,sample)        result.append(temp_func)    return np.array(result)def d_V(x,x_pred = X_pred ,weight = weight,Dx = Dx):    # d(ln(D(x)))/dm1 + d(ln(1 - D(G(x))))/dm1 = d(ln(D(x))/d(D(x)) * d(D(x))/dm1+     # d(ln(1 - D(G(x))))/d(1 - D(G(x))) * d(1 - D(G(x)))/d(D(G(x))) * d(D(G(x)))/dm1    d_m1 = 1/Dx(x) * multi_Dm_1(a[0],X) + 1/(1-Dx(x_pred)) * (-1) * multi_Dm_1(a[0],x_pred)        # d(ln(D(x)))/dm2 + d(ln(1 - D(G(x))))/dm2 = d(ln(D(x))/d(D(x)) * d(D(x))/dm2+     # d(ln(1 - D(G(x))))/d(1 - D(G(x))) * d(1 - D(G(x)))/d(D(G(x))) * d(D(G(x)))/dm2    d_m2 = 1/Dx(x) * multi_Dm_2(a[1],X) + 1/(1-Dx(x_pred)) * (-1) * multi_Dm_2(a[1],x_pred)    # sum and apply weight    result = np.dot([d_m1,d_m2],weight)    return resultupdate = d_V(X)a_new = a + n*updateprint(a_new)Output[0.13001361 0.15280747]GAN ProblemsNone-convergenceDiminished gradient"
  },
  
  {
    "title": "Deep Discriminative Neural Networks",
    "url": "/posts/Deep-Discriminative-Neural-Networks/",
    "categories": "Deep Learning, Deep Discriminative Neural Networks",
    "tags": "Deep Learning, Deep Discriminative Neural Networks",
    "date": "2022-03-01 07:32:00 +0800",
    





    "snippet": "Activation Functions with Non-Vanishing DerivativesActivate Functions Python code# Activate Functionsimport numpy as npdef relu(input):    return (np.abs(input) + input)/2def l_relu(input):    return np.where(input &gt; 0, input, input * 0.1) def tanh(input):    ex = np.exp(input)    enx = np.exp(-input)    return (ex - enx) / (ex + enx)def d_tanh(input):    # 4e^(-2x)/(1+e^(-2x))^2    return 4*np.exp(-2*input)/((1+np.exp(-2*input))*(1+np.exp(-2*input)))def heaviside(input):    threshold = 0.1    return np.heaviside(input-threshold,0.5) net = [[1,0.5,0.2],[-1,-0.5,-0.2],[0.1,-0.1,0]]print(heaviside(np.array(net)))Output[[1.  1.  1. ][0.  0.  0. ][0.5 0.  0. ]]Better Ways to Initialise WeightsAdaptive Versions of BackpropagationBatch NormalisationBatch normalisation# Batch normalisationX = [    [[-0.3,-0.3,-0.8],[0.7,0.6,0.5],[-0.4,0.2,-0.3]],    [[-0.5,0.1,0.8],[-0.5,-0.6,0],[0.7,-0.5,0.6]],    [[0.8,-0.2,0.7],[0.4,0.0,0.0],[0.0,0.1,-0.6]]    ]def batch_normal(input):    input = np.array(input)    # β = 0    a = 0.1    # γ = 1    b = 0.4    # ε = 0.1    c = 0.2    (m,n) = np.shape(input[0])    for x in range(m):        for y in range(n):            temp = np.copy(input[:,x,y])            for i in range(len(input)):                # the function                 input[i,x,y] = a + b*(temp[i] - np.mean(temp))/(np.sqrt(np.var(temp)+c))    return inputprint(np.round(batch_normal(X),4))Output[[[-0.0654 -0.0393 -0.3819][ 0.3949  0.4618  0.3638][-0.2136  0.2962 -0.018 ]][[-0.1756  0.2951  0.3643][-0.3128 -0.2618 -0.0319][ 0.4764 -0.2188  0.5128]][[ 0.5409  0.0443  0.3177][ 0.218   0.1    -0.0319][ 0.0373  0.2226 -0.1949]]]Convolutional Neural NetworksConvolutional Neural Networks Forward python code# Convolutional Neural Networks Forwardimport torchX = [    [[0.2,1,0],    [-1,0,-0.1],    [0.1,0,0.1]],    [[1,0.5,0.2],    [-1,-0.5,-0.2],    [0.1,-0.1,0]]]H = [    [[1,-0.1],    [1,-0.1]],    [[0.5,0.5],    [-0.5,-0.5]]]           x = torch.tensor([X])y = torch.tensor([H])print(torch.nn.functional.conv2d(x,y,stride=1,padding=0, dilation=1))Outputtensor([[[[ 0.6000,  1.7100],        [-1.6500, -0.3000]]]])CNN output dimensioninputDim = 200maskDim = 5padding = 0stride = 1chanel = 40outputDim = 1 + (inputDim - maskDim + 2*padding)/stride print('Dimension:',outputDim,'x',outputDim,'x',chanel)OutputDimension: 196.0 x 196.0 x 40Pooling Layersavg pooling, max pooling python code# avg pooling, max poolingX = [[0.2,1,0,0.4],[-1,0,-0.1,-0.1],[0.1,0,-1,-0.5],[0.4,-0.7,-0.5,1]]x = torch.tensor([[X]])print(torch.nn.functional.avg_pool2d(x,kernel_size =[2,2],stride=2,padding=0))print(torch.nn.functional.max_pool2d(x,kernel_size =[3,3],stride=1,padding=0))Outputtensor([[[[ 0.0500,  0.0500],        [-0.0500, -0.2500]]]])tensor([[[[1.0000, 1.0000],        [0.4000, 1.0000]]]])Fully Connected Layersfinal networkLimitations of Deep NNsVolume of Training DataOverfittingFailure to Generalise"
  },
  
  {
    "title": "Multilayer Perceptrons and Backpropagation",
    "url": "/posts/Multilayer-Perceptrons-and-Backpropagation/",
    "categories": "Deep Learning, Multilayer Perceptrons and Backpropagation",
    "tags": "Deep Learning, Multilayer Perceptrons and Backpropagation",
    "date": "2022-02-21 07:32:00 +0800",
    





    "snippet": "Feedforward Neural NetworksFeedforward Neural Networks Python code# Feedforward Neural Networksimport numpy as npfrom prettytable import PrettyTableimport mathnp.set_printoptions(suppress=True)def log_sigmod(input):    return 1. / (1. + np.exp(-input))def d_log_sigmod(input):    # e^(-x)/(1+e^(-x))^2    return np.exp(-input)/((1+np.exp(-input))*(1+np.exp(-input)))def sym_sigmod(input):    ex = np.exp(input)    enx = np.exp(-input)    return (ex - enx) / (ex + enx)def d_sym_sigmod(input):    # 4e^(-2x)/(1+e^(-2x))^2    return 4*np.exp(-2*input)/((1+np.exp(-2*input))*(1+np.exp(-2*input)))def linear(input):    return inputdef cost(y_pred,y):    return 0.5*(y_pred-y)*(y_pred-y)def d_cost(y_pred,y):    return y_pred - y# input dataX = [[0.4,-0.4]]w1 = [    [5,3],    [-5,-5],    [5,-2]    ]b1 = [-1,-3,4]w2 = [    [5,-1,-5],    [5,1,-1]    ]b2 = [-4,1]# input layer to hidden layery1 = sym_sigmod(np.dot(X,np.transpose(w1)) + b1)z = np.dot(y1,np.transpose(w2)) + b2print(np.dot(X,np.transpose(w1)) + b1)print(np.round(y1,4))print(np.round(z,4))print(np.sum(np.power(z[0] - np.array([-5,5]),2)/2))Output[[-0.2 -3.   6.8]][[-0.1974 -0.9951  1.    ]][[-8.9918 -1.9819]]32.34093671219539Backpropagation AlgorithmBackpropagation Algorithm Python Code# Backpropagation Algorithmx = [0.4,-0.4]net = -1.0633y_pred = -8.9918y = -5m = -0.1974# learning raten = 0.3# d_m = ∂cost/∂m = ∂cost/∂f(mx+b) * ∂f(mx+b)/∂(mx+b) * ∂(mx+b)/∂md_m = d_cost(y_pred,y)*d_sym_sigmod(net)*netm_new = -n*d_m + mresult = np.round([(net,y,y_pred,d_cost(y_pred,y),1,net,d_m,m_new)],4)pt = PrettyTable(('net','y','y_pred','∂cost/∂f(wx+b)','∂f(wx+b)/∂(wx+b)','∂(wx+b)/∂w','d_m','m_new'))for row in result: pt.add_row(row)print(pt)y_2 = -2m_2 = 1.6w = 5a = 0.4# d_w = ∂cost/∂w = ∂cost/∂f(m*g(wx+b)+b) * ∂f(m*g(wx+b)+b)/∂(m*g(wx+b)+b) * ∂(m*g(wx+b)+b)/∂g(wx+b) * ∂(g(wx+b))/∂(wx+b) * ∂(wx+b)/∂wd_w = d_cost(y_pred,y)*m_2 * d_sym_sigmod(-0.2) * x[0]w_new = -n*d_w + wresult = np.round([(net,y,y_pred,d_cost(y_pred,y),d_sym_sigmod(net),m_2,d_sym_sigmod(a),x[0],d_w,w_new)],4)pt = PrettyTable(('net','y','y_pred','∂cost/∂f(m*g(wx+b)+b)','∂f(m*g(wx+b)+b)/∂(m*g(wx+b)+b)','∂(m*g(wx+b)+b)/∂g(wx+b)','∂(g(wx+b))/∂(wx+b)','∂(wx+b)/∂w','d_w','w_new'))for row in result: pt.add_row(row)print(pt)Output+---------+------+---------+----------------+------------------+------------+--------+---------+|   net   |  y   |  y_pred | ∂cost/∂f(wx+b) | ∂f(wx+b)/∂(wx+b) | ∂(wx+b)/∂w |  d_m   |  m_new  |+---------+------+---------+----------------+------------------+------------+--------+---------+| -1.0633 | -5.0 | -8.9918 |    -3.9918     |       1.0        |  -1.0633   | 1.6161 | -0.6822 |+---------+------+---------+----------------+------------------+------------+--------+---------++---------+------+---------+-----------------------+--------------------------------+-------------------------+--------------------+------------+---------+--------+|   net   |  y   |  y_pred | ∂cost/∂f(m*g(wx+b)+b) | ∂f(m*g(wx+b)+b)/∂(m*g(wx+b)+b) | ∂(m*g(wx+b)+b)/∂g(wx+b) | ∂(g(wx+b))/∂(wx+b) | ∂(wx+b)/∂w |   d_w   | w_new  |+---------+------+---------+-----------------------+--------------------------------+-------------------------+--------------------+------------+---------+--------+| -1.0633 | -5.0 | -8.9918 |        -3.9918        |             0.3808             |           1.6           |       0.8556       |    0.4     | -2.4552 | 5.7366 |+---------+------+---------+-----------------------+--------------------------------+-------------------------+--------------------+------------+---------+--------+RBF network output unit Python Code# RBF network output unitw = [-2.5027,-2.5027]b = 2.8413z = np.dot(np.transpose(y),np.transpose([w])) + bnp.set_printoptions(suppress=True)print(z)Output[[-0.00010361][ 0.99991625][ 0.99991625][-0.00010361]]Continue Python Code# m * w = z, get wm = np.transpose(np.append(y,np.array([[1,1,1,1]]),axis=0))z = np.array([[0,1,1,0]])print('Z:\\n',z)print('Y:\\n',m)w = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(m),m)),np.transpose(m)),np.transpose(z))print('W:\\n',w)OutputZ:[[0 1 1 0]]Y:[[1.         0.13533528 1.        ][0.36787944 0.36787944 1.        ][0.36787944 0.36787944 1.        ][0.13533528 1.         1.        ]]W:[[-2.5026503 ][-2.5026503 ][ 2.84134719]]"
  },
  
  {
    "title": "Neural Networks",
    "url": "/posts/Neural-Networks/",
    "categories": "Deep Learning, Neural Networks",
    "tags": "Deep Learning, Neural Networks",
    "date": "2022-02-11 07:32:00 +0800",
    





    "snippet": "Neural NetworksLinear Threshold Unit / PerceptronLinear Threshold Unit Python code# Linear Threshold Unitdef h_func(x):    if x &lt; 0:        return 0    else:        return 1x = [[0,0],[0,1],[1,0],[1,1]]# logical ANDb = 1.5# w = np.ones(len(x))# logical NOT# b = -0.5# w = np.ones(len(x)) * (-1)# logical OR# b = 0.5# w = np.ones(len(x))# define ww = [1,1]result = []for sample in x:    temp = np.dot(w,sample) - b    y = h_func(temp)    result.append((sample,np.dot(w,sample),temp,y))pt = PrettyTable(('Sample','wx','wx-b','h(wx-b)'))for row in result: pt.add_row(row)print(pt)Output+--------+----+------+---------+| Sample | wx | wx-b | h(wx-b) |+--------+----+------+---------+| [0, 0] | 0  | -1.5 |    0    || [0, 1] | 1  | -0.5 |    0    || [1, 0] | 1  | -0.5 |    0    || [1, 1] | 2  | 0.5  |    1    |+--------+----+------+---------+Delta Learning RuleDelta Learning Rule Python Code# Delta Learning Ruleimport numpy as npfrom prettytable import PrettyTable# configuration variables# -----------------------------------------------------------# initial values# w = [b,w0,w1]# H(wx - theta)# w = [-theta,w0,w1]theta = 1.5w = [-theta,5,-1]# learning raten = 1def H(input):    if input &lt; 0:        return 0    else:        return 1# iterations# iterations = 150epoch = 3# dataset# -----------------------------------------------------------X = [[0,0],[1,0],[2,1],[0,1],[1,2]]Y = [1,1,1,0,0]# sequential widrow-hoff learning algorithm# -----------------------------------------------------------result = []for o in range(epoch):    for i in range(len(Y)):        w_prev = w        x = np.hstack((np.array(1),X[i]))        y = Y[i]        # calculate wx        wx = np.dot(w, x)        # calculate update part        temp =  H(wx)        update = n * (y - H(wx)) * x        # add update part to a        w = np.add(w, update)        cur_result = []        # evaluate        for index in range(len(Y)):            y_1 = np.hstack((1,X[index]))            cur_result.append(H(np.dot(w, y_1)))        # check if converage        is_converage = True        for index in range(len(Y)):            if cur_result[index] != Y[index]:                is_converage = False                        # append result        result.append((str(i + 1 + (len(Y) * o)),x, np.round(w_prev, 4), np.round(y, 4), np.round(wx, 4),y,temp,update, np.round(w, 4),np.round(cur_result, 4),is_converage))# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration','x', 'w', 't', 'wx','y = h(wx)','t-y','η(t-y)x', 'w_new = w + η(t - y)x','over all result','is converage'))for row in result: pt.add_row(row)print(pt)Output+-----------+---------+------------------+---+------+-----------+-----+------------+-----------------------+-----------------+--------------+| iteration |    x    |        w         | t |  wx  | y = h(wx) | t-y |  η(t-y)x   | w_new = w + η(t - y)x | over all result | is converage |+-----------+---------+------------------+---+------+-----------+-----+------------+-----------------------+-----------------+--------------+|     1     | [1 0 0] | [-1.5  5.  -1. ] | 1 | -1.5 |     1     |  0  |  [1 0 0]   |    [-0.5  5.  -1. ]   |   [0 1 1 0 1]   |    False     ||     2     | [1 1 0] | [-0.5  5.  -1. ] | 1 | 4.5  |     1     |  1  |  [0 0 0]   |    [-0.5  5.  -1. ]   |   [0 1 1 0 1]   |    False     ||     3     | [1 2 1] | [-0.5  5.  -1. ] | 1 | 8.5  |     1     |  1  |  [0 0 0]   |    [-0.5  5.  -1. ]   |   [0 1 1 0 1]   |    False     ||     4     | [1 0 1] | [-0.5  5.  -1. ] | 0 | -1.5 |     0     |  0  |  [0 0 0]   |    [-0.5  5.  -1. ]   |   [0 1 1 0 1]   |    False     ||     5     | [1 1 2] | [-0.5  5.  -1. ] | 0 | 2.5  |     0     |  1  | [-1 -1 -2] |    [-1.5  4.  -3. ]   |   [0 1 1 0 0]   |    False     ||     6     | [1 0 0] | [-1.5  4.  -3. ] | 1 | -1.5 |     1     |  0  |  [1 0 0]   |    [-0.5  4.  -3. ]   |   [0 1 1 0 0]   |    False     ||     7     | [1 1 0] | [-0.5  4.  -3. ] | 1 | 3.5  |     1     |  1  |  [0 0 0]   |    [-0.5  4.  -3. ]   |   [0 1 1 0 0]   |    False     ||     8     | [1 2 1] | [-0.5  4.  -3. ] | 1 | 4.5  |     1     |  1  |  [0 0 0]   |    [-0.5  4.  -3. ]   |   [0 1 1 0 0]   |    False     ||     9     | [1 0 1] | [-0.5  4.  -3. ] | 0 | -3.5 |     0     |  0  |  [0 0 0]   |    [-0.5  4.  -3. ]   |   [0 1 1 0 0]   |    False     ||     10    | [1 1 2] | [-0.5  4.  -3. ] | 0 | -2.5 |     0     |  0  |  [0 0 0]   |    [-0.5  4.  -3. ]   |   [0 1 1 0 0]   |    False     ||     11    | [1 0 0] | [-0.5  4.  -3. ] | 1 | -0.5 |     1     |  0  |  [1 0 0]   |    [ 0.5  4.  -3. ]   |   [1 1 1 0 0]   |     True     ||     12    | [1 1 0] | [ 0.5  4.  -3. ] | 1 | 4.5  |     1     |  1  |  [0 0 0]   |    [ 0.5  4.  -3. ]   |   [1 1 1 0 0]   |     True     ||     13    | [1 2 1] | [ 0.5  4.  -3. ] | 1 | 5.5  |     1     |  1  |  [0 0 0]   |    [ 0.5  4.  -3. ]   |   [1 1 1 0 0]   |     True     ||     14    | [1 0 1] | [ 0.5  4.  -3. ] | 0 | -2.5 |     0     |  0  |  [0 0 0]   |    [ 0.5  4.  -3. ]   |   [1 1 1 0 0]   |     True     ||     15    | [1 1 2] | [ 0.5  4.  -3. ] | 0 | -1.5 |     0     |  0  |  [0 0 0]   |    [ 0.5  4.  -3. ]   |   [1 1 1 0 0]   |     True     |+-----------+---------+------------------+---+------+-----------+-----+------------+-----------------------+-----------------+--------------+Softmax Python Code# softmaximport mathy = [0.34,0.73,-0.16]# β hyperparameterb = 1total = 0for sample in y:    total += math.exp(sample*b)result = []for sample in y:    result.append((sample,np.round(math.exp(sample*b)/total, 4)))pt = PrettyTable(('Sample','softmax'))for row in result: pt.add_row(row)print(pt)Output+--------+---------+| Sample | softmax |+--------+---------+|  0.34  |  0.3243 ||  0.73  |  0.479  || -0.16  |  0.1967 |+--------+---------+Hebbian Learning RuleCompetitive Learning NetworksNegative Feedback NetworksNegative feedback network Python code# negative feedback networkfrom prettytable import PrettyTablefrom fractions import Fraction# w and x stay unchangew = [[1,1,0],[1,1,1]]x = [1,1,0]y = [0,0]# parameter α = 0.25b = 0.25iternation = 10result = []for i in range(iternation):    w_prev = w    wy = np.dot(np.transpose(w),y)    e = x - wy    we = np.dot(w,e)    y = y + b*we    # append result    result.append((str(i + 1),y,np.round(wy,4), np.round(e, 4), np.round(we, 4),np.round(y,4)))pt = PrettyTable(('iteration','y','wy_new','e=x-wy','we','y_new = y + b*we'))for row in result: pt.add_row(row)print(pt)Output+-----------+-------------------------+------------------------+---------------------------+-------------------+------------------+| iteration |            y            |         wy_new         |           e=x-wy          |         we        | y_new = y + b*we |+-----------+-------------------------+------------------------+---------------------------+-------------------+------------------+|     1     |        [0.5 0.5]        |        [0 0 0]         |          [1 1 0]          |       [2 2]       |    [0.5 0.5]     ||     2     |      [0.5   0.375]      |     [1.  1.  0.5]      |      [ 0.   0.  -0.5]     |    [ 0.  -0.5]    |  [0.5   0.375]   ||     3     |    [0.5625  0.34375]    |  [0.875 0.875 0.375]   |   [ 0.125  0.125 -0.375]  |  [ 0.25  -0.125]  | [0.5625 0.3438]  ||     4     |  [0.609375  0.3046875]  | [0.9062 0.9062 0.3438] | [ 0.0938  0.0938 -0.3438] | [ 0.1875 -0.1562] | [0.6094 0.3047]  ||     5     | [0.65234375 0.27148438] | [0.9141 0.9141 0.3047] | [ 0.0859  0.0859 -0.3047] | [ 0.1719 -0.1328] | [0.6523 0.2715]  ||     6     | [0.69042969 0.24169922] | [0.9238 0.9238 0.2715] | [ 0.0762  0.0762 -0.2715] | [ 0.1523 -0.1191] | [0.6904 0.2417]  ||     7     | [0.72436523 0.21520996] | [0.9321 0.9321 0.2417] | [ 0.0679  0.0679 -0.2417] | [ 0.1357 -0.106 ] | [0.7244 0.2152]  ||     8     | [0.75457764 0.19161987] | [0.9396 0.9396 0.2152] | [ 0.0604  0.0604 -0.2152] | [ 0.1208 -0.0944] | [0.7546 0.1916]  ||     9     | [0.78147888 0.17061615] | [0.9462 0.9462 0.1916] | [ 0.0538  0.0538 -0.1916] | [ 0.1076 -0.084 ] | [0.7815 0.1706]  ||     10    | [0.80543137 0.1519146 ] | [0.9521 0.9521 0.1706] | [ 0.0479  0.0479 -0.1706] | [ 0.0958 -0.0748] | [0.8054 0.1519]  |+-----------+-------------------------+------------------------+---------------------------+-------------------+------------------+Autoencoder Networks"
  },
  
  {
    "title": "Discriminant Functions",
    "url": "/posts/Discriminant-Functions/",
    "categories": "Deep Learning, Discriminant Functions",
    "tags": "Deep Learning, Discriminant Functions",
    "date": "2022-02-02 07:32:00 +0800",
    





    "snippet": "Discriminant functions divide feature space into regionsLinear Discriminant FunctionsGeneralised Linear Discriminant FunctionsDichotomizer Python codeimport numpy as npX = [    [1,1],    [2,2],    [3,3]]w = [2,1]b = -5def g(w,x,b):    return np.dot(w,x) + bresult = [g(w,x,b) for x in X]print(result)Output[-2, 1, 4]Learning Decision Boundariesperceptron learningPerceptron Criterion FunctionSequential Perceptron Learning AlgorithmSequential Perceptron Learning Algorithm Python code# Sequential Perceptron Learning Algorithmimport numpy as npfrom prettytable import PrettyTable# configuration variables# -----------------------------------------------------------# initial values# a = [b, w1, w2]a = [-1.5,5,-1]# learning raten = 1epoch = 3# dataset# -----------------------------------------------------------X = [[0,0],[1,0],[2,1],[0,1],[1,2]]Y = [1,1,1,-1,-1]# Sequential Perceptron Learning Algorithm# -----------------------------------------------------------result = []for o in range(epoch):    for i in range(len(Y)):        a_prev = a        # y = [1, x1, x2]        y = np.hstack((1,X[i]))        # calculate ay like wx+b = b + w1x1 + w2x2        ay = np.dot(a, y)        #update part ηω'yk        if ay * Y[i] &lt; 0:            if ay &gt; 0:                update = n * y * (-1)            if ay &lt; 0:                update = n * y             # add update part to a            a = np.add(a, update)        cur_result = []        # evaluate        for index in range(len(Y)):            cur_result.append((np.dot(np.hstack((1,X[index])),a)))        # check if converage        is_converage = True        for index in range(len(Y)):            if cur_result[index] * Y[index] &lt;= 0:                is_converage = False                    # append result        result.append((str(i + 1 + (len(Y) * o)), np.round(a_prev, 4), np.round(y, 4), np.round(ay, 4), np.round(a, 4),np.round(cur_result, 4),is_converage))# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration', 'a', 'y', 'ay', 'a_new','over all result','is converage'))for row in result: pt.add_row(row)print(pt)Output+-----------+------------------+---------+------+------------------+----------------------------+-------------+| iteration | a                | y       | ay   | a_new            |      over all result       | is converge |+-----------+------------------+---------+------+------------------+----------------------------+-------------+|     1     | [-1.5  5.  -1. ] | [1 0 0] | -1.5 | [-0.5  5.  -1. ] | [-0.5  4.5  8.5 -1.5  2.5] |    False    ||     2     | [-0.5  5.  -1. ] | [1 1 0] | 4.5  | [-0.5  5.  -1. ] | [-0.5  4.5  8.5 -1.5  2.5] |    False    ||     3     | [-0.5  5.  -1. ] | [1 2 1] | 8.5  | [-0.5  5.  -1. ] | [-0.5  4.5  8.5 -1.5  2.5] |    False    ||     4     | [-0.5  5.  -1. ] | [1 0 1] | -1.5 | [-0.5  5.  -1. ] | [-0.5  4.5  8.5 -1.5  2.5] |    False    ||     5     | [-0.5  5.  -1. ] | [1 1 2] | 2.5  | [-1.5  4.  -3. ] | [-1.5  2.5  3.5 -4.5 -3.5] |    False    ||     6     | [-1.5  4.  -3. ] | [1 0 0] | -1.5 | [-0.5  4.  -3. ] | [-0.5  3.5  4.5 -3.5 -2.5] |    False    ||     7     | [-0.5  4.  -3. ] | [1 1 0] | 3.5  | [-0.5  4.  -3. ] | [-0.5  3.5  4.5 -3.5 -2.5] |    False    ||     8     | [-0.5  4.  -3. ] | [1 2 1] | 4.5  | [-0.5  4.  -3. ] | [-0.5  3.5  4.5 -3.5 -2.5] |    False    ||     9     | [-0.5  4.  -3. ] | [1 0 1] | -3.5 | [-0.5  4.  -3. ] | [-0.5  3.5  4.5 -3.5 -2.5] |    False    ||     10    | [-0.5  4.  -3. ] | [1 1 2] | -2.5 | [-0.5  4.  -3. ] | [-0.5  3.5  4.5 -3.5 -2.5] |    False    ||     11    | [-0.5  4.  -3. ] | [1 0 0] | -0.5 | [ 0.5  4.  -3. ] | [ 0.5  4.5  5.5 -2.5 -1.5] |     True    ||     12    | [ 0.5  4.  -3. ] | [1 1 0] | 4.5  | [ 0.5  4.  -3. ] | [ 0.5  4.5  5.5 -2.5 -1.5] |     True    ||     13    | [ 0.5  4.  -3. ] | [1 2 1] | 5.5  | [ 0.5  4.  -3. ] | [ 0.5  4.5  5.5 -2.5 -1.5] |     True    ||     14    | [ 0.5  4.  -3. ] | [1 0 1] | -2.5 | [ 0.5  4.  -3. ] | [ 0.5  4.5  5.5 -2.5 -1.5] |     True    ||     15    | [ 0.5  4.  -3. ] | [1 1 2] | -1.5 | [ 0.5  4.  -3. ] | [ 0.5  4.5  5.5 -2.5 -1.5] |     True    |+-----------+------------------+---------+------+------------------+----------------------------+-------------+Batch Perceptron Learning Algorithm# Batch Perceptron Learning Algorithmimport numpy as npfrom prettytable import PrettyTable# configuration variables# -----------------------------------------------------------# initial values# a = [b, w1, w2]a = [-25,6,3]# learning raten = 1epoch = 3# dataset# -----------------------------------------------------------X = [[1,5],[2,5],[4,1],[5,1]]Y = [1,1,-1,-1]# Sequential Perceptron Learning Algorithm# -----------------------------------------------------------result = []for o in range(epoch):    update = 0    for i in range(len(Y)):        a_prev = a        # y = [1, x1, x2]        y = np.hstack((1,X[i]))        # calculate ay like wx+b = b + w1x1 + w2x2        ay = np.dot(a, y)        #update part ηω'yk        if ay * Y[i] &lt;= 0:            if ay &gt; 0:                update = update + n * y * (-1)            if ay &lt;= 0:                update = update + n * y         cur_result = []        # evaluate        for index in range(len(Y)):            cur_result.append((np.dot(np.hstack((1,X[index])),a)))        # check if converage        is_converage = True        for index in range(len(Y)):            if cur_result[index] * Y[index] &lt;= 0:                is_converage = False                    # append result        result.append((str(i + 1 + (len(Y) * o)), np.round(a_prev, 4), np.round(y, 4), np.round(ay, 4), np.round(a, 4),np.round(cur_result, 4),is_converage))    # add update part to a    a = np.add(a, update)# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration', 'a', 'y', 'ay', 'a_new','over all result','is converage'))for row in result: pt.add_row(row)print(pt)Output+-----------+---------------+---------+-----+---------------+-------------------+--------------+| iteration |       a       |    y    |  ay |     a_new     |  over all result  | is converage |+-----------+---------------+---------+-----+---------------+-------------------+--------------+|     1     | [-25   6   3] | [1 1 5] |  -4 | [-25   6   3] |   [-4  2  2  8]   |    False     ||     2     | [-25   6   3] | [1 2 5] |  2  | [-25   6   3] |   [-4  2  2  8]   |    False     ||     3     | [-25   6   3] | [1 4 1] |  2  | [-25   6   3] |   [-4  2  2  8]   |    False     ||     4     | [-25   6   3] | [1 5 1] |  8  | [-25   6   3] |   [-4  2  2  8]   |    False     ||     5     | [-26  -2   6] | [1 1 5] |  2  | [-26  -2   6] | [  2   0 -28 -30] |    False     ||     6     | [-26  -2   6] | [1 2 5] |  0  | [-26  -2   6] | [  2   0 -28 -30] |    False     ||     7     | [-26  -2   6] | [1 4 1] | -28 | [-26  -2   6] | [  2   0 -28 -30] |    False     ||     8     | [-26  -2   6] | [1 5 1] | -30 | [-26  -2   6] | [  2   0 -28 -30] |    False     ||     9     | [-25   0  11] | [1 1 5] |  30 | [-25   0  11] | [ 30  30 -14 -14] |     True     ||     10    | [-25   0  11] | [1 2 5] |  30 | [-25   0  11] | [ 30  30 -14 -14] |     True     ||     11    | [-25   0  11] | [1 4 1] | -14 | [-25   0  11] | [ 30  30 -14 -14] |     True     ||     12    | [-25   0  11] | [1 5 1] | -14 | [-25   0  11] | [ 30  30 -14 -14] |     True     |+-----------+---------------+---------+-----+---------------+-------------------+--------------+Sequential Multiclass Perceptron Learning algorithm Python code# Sequential Multiclass Perceptron Learning algorithmimport numpy as npfrom prettytable import PrettyTable# configuration variables# -----------------------------------------------------------# initial values# a = [b, w1, w2]a = [[-0.5,0,-0.5],[-3,0.5,0],[0.5,1.5,-0.5]]# learning raten = 1epoch = 5# dataset# -----------------------------------------------------------X = [[0,1],[1,0],[0.5,1.5],[1,1],[-0.5,0]]Y = [1,1,2,2,3]# Sequential Perceptron Learning Algorithm# -----------------------------------------------------------result = []for o in range(epoch):    for i in range(len(Y)):        a_prev = a.copy()        # y = [1, x1, x2]        y = np.hstack((1,X[i]))        # calculate ay like wx+b = b + w1x1 + w2x2        ay = np.dot(a, np.transpose([y]))        if all(x == ay[0] for x in ay):            j = 2        else:            j = np.argmax(ay)        true_y = Y[i] - 1        if j != true_y:            update =  n * y             a[true_y] = np.add(a[true_y], update)            update =  n * y * (-1)            a[j] = np.add(a[j], update)        cur_result = []        # evaluate        for index in range(len(Y)):            cur_result.append((np.dot(np.hstack((1,X[index])),np.transpose(a))))        # check if converage        is_converage = False        if np.array_equal(np.array(Y),np.argmax(cur_result,axis=1) + 1):            is_converage = True                    # append result        result.append((str(i + 1 + (len(Y) * o)), np.round(a_prev, 4), np.round(y, 4), np.round(ay, 4),j+1, np.round(a, 4),np.round(cur_result, 4),np.argmax(cur_result,axis=1) + 1,is_converage))# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration', 'a', 'y', 'ay','class', 'a_new','over all result','class result','is converage'))for row in result: pt.add_row(row)print(pt)Output+-----------+--------------------+------------------+-----------+-------+--------------------+-----------------------+--------------+--------------+| iteration |         a          |        y         |     ay    | class |       a_new        |    over all result    | class result | is converage |+-----------+--------------------+------------------+-----------+-------+--------------------+-----------------------+--------------+--------------+|     1     | [[-0.5  0.  -0.5]  |     [1 0 1]      |   [[-1.]  |   3   | [[ 0.5  0.   0.5]  |  [[ 1.   -3.   -2.  ] | [1 3 1 1 1]  |    False     ||           |  [-3.   0.5  0. ]  |                  |    [-3.]  |       |  [-3.   0.5  0. ]  |   [ 0.5  -2.5   1.  ] |              |              ||           |  [ 0.5  1.5 -0.5]] |                  |   [ 0.]]  |       |  [-0.5  1.5 -1.5]] |   [ 1.25 -2.75 -2.  ] |              |              ||           |                    |                  |           |       |                    |   [ 1.   -2.5  -0.5 ] |              |              ||           |                    |                  |           |       |                    |  [ 0.5  -3.25 -1.25]] |              |              ||     2     | [[ 0.5  0.   0.5]  |     [1 1 0]      |  [[ 0.5]  |   3   | [[ 1.5  1.   0.5]  |  [[ 2.   -3.   -3.  ] | [1 1 1 1 1]  |    False     ||           |  [-3.   0.5  0. ]  |                  |   [-2.5]  |       |  [-3.   0.5  0. ]  |   [ 2.5  -2.5  -1.  ] |              |              ||           |  [-0.5  1.5 -1.5]] |                  |   [ 1. ]] |       |  [-1.5  0.5 -1.5]] |   [ 2.75 -2.75 -3.5 ] |              |              ||           |                    |                  |           |       |                    |   [ 3.   -2.5  -2.5 ] |              |              ||           |                    |                  |           |       |                    |  [ 1.   -3.25 -1.75]] |              |              ||     3     | [[ 1.5  1.   0.5]  |  [1.  0.5 1.5]   |  [[ 2.75] |   1   | [[ 0.5  0.5 -1. ]  |  [[-0.5  -0.5  -3.  ] | [1 1 2 2 1]  |    False     ||           |  [-3.   0.5  0. ]  |                  |   [-2.75] |       |  [-2.   1.   1.5]  |   [ 1.   -1.   -1.  ] |              |              ||           |  [-1.5  0.5 -1.5]] |                  |  [-3.5 ]] |       |  [-1.5  0.5 -1.5]] |   [-0.75  0.75 -3.5 ] |              |              ||           |                    |                  |           |       |                    |   [ 0.    0.5  -2.5 ] |              |              ||           |                    |                  |           |       |                    |  [ 0.25 -2.5  -1.75]] |              |              ||     4     | [[ 0.5  0.5 -1. ]  |     [1 1 1]      |  [[ 0. ]  |   2   | [[ 0.5  0.5 -1. ]  |  [[-0.5  -0.5  -3.  ] | [1 1 2 2 1]  |    False     ||           |  [-2.   1.   1.5]  |                  |   [ 0.5]  |       |  [-2.   1.   1.5]  |   [ 1.   -1.   -1.  ] |              |              ||           |  [-1.5  0.5 -1.5]] |                  |   [-2.5]] |       |  [-1.5  0.5 -1.5]] |   [-0.75  0.75 -3.5 ] |              |              ||           |                    |                  |           |       |                    |   [ 0.    0.5  -2.5 ] |              |              ||           |                    |                  |           |       |                    |  [ 0.25 -2.5  -1.75]] |              |              ||     5     | [[ 0.5  0.5 -1. ]  | [ 1.  -0.5  0. ] |  [[ 0.25] |   1   | [[-0.5  1.  -1. ]  |  [[-1.5  -0.5  -2.  ] | [2 1 2 2 3]  |    False     ||           |  [-2.   1.   1.5]  |                  |   [-2.5 ] |       |  [-2.   1.   1.5]  |   [ 0.5  -1.   -0.5 ] |              |              ||           |  [-1.5  0.5 -1.5]] |                  |  [-1.75]] |       |  [-0.5  0.  -1.5]] |   [-1.5   0.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [-0.5   0.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [-1.   -2.5  -0.5 ]] |              |              ||     6     | [[-0.5  1.  -1. ]  |     [1 0 1]      |  [[-1.5]  |   2   | [[ 0.5  1.   0. ]  |  [[ 0.5  -2.5  -2.  ] | [1 1 1 1 1]  |    False     ||           |  [-2.   1.   1.5]  |                  |   [-0.5]  |       |  [-3.   1.   0.5]  |   [ 1.5  -2.   -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |   [-2. ]] |       |  [-0.5  0.  -1.5]] |   [ 1.   -1.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [ 1.5  -1.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [ 0.   -3.5  -0.5 ]] |              |              ||     7     | [[ 0.5  1.   0. ]  |     [1 1 0]      |  [[ 1.5]  |   1   | [[ 0.5  1.   0. ]  |  [[ 0.5  -2.5  -2.  ] | [1 1 1 1 1]  |    False     ||           |  [-3.   1.   0.5]  |                  |   [-2. ]  |       |  [-3.   1.   0.5]  |   [ 1.5  -2.   -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |   [-0.5]] |       |  [-0.5  0.  -1.5]] |   [ 1.   -1.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [ 1.5  -1.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [ 0.   -3.5  -0.5 ]] |              |              ||     8     | [[ 0.5  1.   0. ]  |  [1.  0.5 1.5]   |  [[ 1.  ] |   1   | [[-0.5  0.5 -1.5]  |  [[-2.    0.   -2.  ] | [2 1 2 2 3]  |    False     ||           |  [-3.   1.   0.5]  |                  |   [-1.75] |       |  [-2.   1.5  2. ]  |   [ 0.   -0.5  -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |  [-2.75]] |       |  [-0.5  0.  -1.5]] |   [-2.5   1.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [-1.5   1.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [-0.75 -2.75 -0.5 ]] |              |              ||     9     | [[-0.5  0.5 -1.5]  |     [1 1 1]      |  [[-1.5]  |   2   | [[-0.5  0.5 -1.5]  |  [[-2.    0.   -2.  ] | [2 1 2 2 3]  |    False     ||           |  [-2.   1.5  2. ]  |                  |   [ 1.5]  |       |  [-2.   1.5  2. ]  |   [ 0.   -0.5  -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |   [-2. ]] |       |  [-0.5  0.  -1.5]] |   [-2.5   1.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [-1.5   1.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [-0.75 -2.75 -0.5 ]] |              |              ||     10    | [[-0.5  0.5 -1.5]  | [ 1.  -0.5  0. ] |  [[-0.75] |   3   | [[-0.5  0.5 -1.5]  |  [[-2.    0.   -2.  ] | [2 1 2 2 3]  |    False     ||           |  [-2.   1.5  2. ]  |                  |   [-2.75] |       |  [-2.   1.5  2. ]  |   [ 0.   -0.5  -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |  [-0.5 ]] |       |  [-0.5  0.  -1.5]] |   [-2.5   1.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [-1.5   1.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [-0.75 -2.75 -0.5 ]] |              |              ||     11    | [[-0.5  0.5 -1.5]  |     [1 0 1]      |   [[-2.]  |   2   | [[ 0.5  0.5 -0.5]  |  [[ 0.   -2.   -2.  ] | [1 1 1 1 1]  |    False     ||           |  [-2.   1.5  2. ]  |                  |    [ 0.]  |       |  [-3.   1.5  1. ]  |   [ 1.   -1.5  -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |   [-2.]]  |       |  [-0.5  0.  -1.5]] |   [ 0.   -0.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [ 0.5  -0.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [ 0.25 -3.75 -0.5 ]] |              |              ||     12    | [[ 0.5  0.5 -0.5]  |     [1 1 0]      |  [[ 1. ]  |   1   | [[ 0.5  0.5 -0.5]  |  [[ 0.   -2.   -2.  ] | [1 1 1 1 1]  |    False     ||           |  [-3.   1.5  1. ]  |                  |   [-1.5]  |       |  [-3.   1.5  1. ]  |   [ 1.   -1.5  -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |   [-0.5]] |       |  [-0.5  0.  -1.5]] |   [ 0.   -0.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [ 0.5  -0.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [ 0.25 -3.75 -0.5 ]] |              |              ||     13    | [[ 0.5  0.5 -0.5]  |  [1.  0.5 1.5]   |  [[ 0.  ] |   1   | [[-0.5  0.  -2. ]  |  [[-2.5   0.5  -2.  ] | [2 2 2 2 1]  |    False     ||           |  [-3.   1.5  1. ]  |                  |   [-0.75] |       |  [-2.   2.   2.5]  |   [-0.5   0.   -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |  [-2.75]] |       |  [-0.5  0.  -1.5]] |   [-3.5   2.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [-2.5   2.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [-0.5  -3.   -0.5 ]] |              |              ||     14    | [[-0.5  0.  -2. ]  |     [1 1 1]      |  [[-2.5]  |   2   | [[-0.5  0.  -2. ]  |  [[-2.5   0.5  -2.  ] | [2 2 2 2 1]  |    False     ||           |  [-2.   2.   2.5]  |                  |   [ 2.5]  |       |  [-2.   2.   2.5]  |   [-0.5   0.   -0.5 ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |   [-2. ]] |       |  [-0.5  0.  -1.5]] |   [-3.5   2.75 -2.75] |              |              ||           |                    |                  |           |       |                    |   [-2.5   2.5  -2.  ] |              |              ||           |                    |                  |           |       |                    |  [-0.5  -3.   -0.5 ]] |              |              ||     15    | [[-0.5  0.  -2. ]  | [ 1.  -0.5  0. ] |  [[-0.5]  |   1   | [[-1.5  0.5 -2. ]  |  [[-3.5   0.5  -1.  ] | [2 2 2 2 3]  |    False     ||           |  [-2.   2.   2.5]  |                  |   [-3. ]  |       |  [-2.   2.   2.5]  |   [-1.    0.    0.  ] |              |              ||           |  [-0.5  0.  -1.5]] |                  |   [-0.5]] |       |  [ 0.5 -0.5 -1.5]] |   [-4.25  2.75 -2.  ] |              |              ||           |                    |                  |           |       |                    |   [-3.    2.5  -1.5 ] |              |              ||           |                    |                  |           |       |                    |  [-1.75 -3.    0.75]] |              |              ||     16    | [[-1.5  0.5 -2. ]  |     [1 0 1]      |  [[-3.5]  |   2   | [[-0.5  0.5 -1. ]  |  [[-1.5  -1.5  -1.  ] | [3 1 2 2 3]  |    False     ||           |  [-2.   2.   2.5]  |                  |   [ 0.5]  |       |  [-3.   2.   1.5]  |   [ 0.   -1.    0.  ] |              |              ||           |  [ 0.5 -0.5 -1.5]] |                  |   [-1. ]] |       |  [ 0.5 -0.5 -1.5]] |   [-1.75  0.25 -2.  ] |              |              ||           |                    |                  |           |       |                    |   [-1.    0.5  -1.5 ] |              |              ||           |                    |                  |           |       |                    |  [-0.75 -4.    0.75]] |              |              ||     17    | [[-0.5  0.5 -1. ]  |     [1 1 0]      |   [[ 0.]  |   1   | [[-0.5  0.5 -1. ]  |  [[-1.5  -1.5  -1.  ] | [3 1 2 2 3]  |    False     ||           |  [-3.   2.   1.5]  |                  |    [-1.]  |       |  [-3.   2.   1.5]  |   [ 0.   -1.    0.  ] |              |              ||           |  [ 0.5 -0.5 -1.5]] |                  |   [ 0.]]  |       |  [ 0.5 -0.5 -1.5]] |   [-1.75  0.25 -2.  ] |              |              ||           |                    |                  |           |       |                    |   [-1.    0.5  -1.5 ] |              |              ||           |                    |                  |           |       |                    |  [-0.75 -4.    0.75]] |              |              ||     18    | [[-0.5  0.5 -1. ]  |  [1.  0.5 1.5]   |  [[-1.75] |   2   | [[-0.5  0.5 -1. ]  |  [[-1.5  -1.5  -1.  ] | [3 1 2 2 3]  |    False     ||           |  [-3.   2.   1.5]  |                  |   [ 0.25] |       |  [-3.   2.   1.5]  |   [ 0.   -1.    0.  ] |              |              ||           |  [ 0.5 -0.5 -1.5]] |                  |  [-2.  ]] |       |  [ 0.5 -0.5 -1.5]] |   [-1.75  0.25 -2.  ] |              |              ||           |                    |                  |           |       |                    |   [-1.    0.5  -1.5 ] |              |              ||           |                    |                  |           |       |                    |  [-0.75 -4.    0.75]] |              |              ||     19    | [[-0.5  0.5 -1. ]  |     [1 1 1]      |  [[-1. ]  |   2   | [[-0.5  0.5 -1. ]  |  [[-1.5  -1.5  -1.  ] | [3 1 2 2 3]  |    False     ||           |  [-3.   2.   1.5]  |                  |   [ 0.5]  |       |  [-3.   2.   1.5]  |   [ 0.   -1.    0.  ] |              |              ||           |  [ 0.5 -0.5 -1.5]] |                  |   [-1.5]] |       |  [ 0.5 -0.5 -1.5]] |   [-1.75  0.25 -2.  ] |              |              ||           |                    |                  |           |       |                    |   [-1.    0.5  -1.5 ] |              |              ||           |                    |                  |           |       |                    |  [-0.75 -4.    0.75]] |              |              ||     20    | [[-0.5  0.5 -1. ]  | [ 1.  -0.5  0. ] |  [[-0.75] |   3   | [[-0.5  0.5 -1. ]  |  [[-1.5  -1.5  -1.  ] | [3 1 2 2 3]  |    False     ||           |  [-3.   2.   1.5]  |                  |   [-4.  ] |       |  [-3.   2.   1.5]  |   [ 0.   -1.    0.  ] |              |              ||           |  [ 0.5 -0.5 -1.5]] |                  |  [ 0.75]] |       |  [ 0.5 -0.5 -1.5]] |   [-1.75  0.25 -2.  ] |              |              ||           |                    |                  |           |       |                    |   [-1.    0.5  -1.5 ] |              |              ||           |                    |                  |           |       |                    |  [-0.75 -4.    0.75]] |              |              ||     21    | [[-0.5  0.5 -1. ]  |     [1 0 1]      |  [[-1.5]  |   3   | [[ 0.5  0.5  0. ]  |  [[ 0.5  -1.5  -3.  ] | [1 1 1 1 1]  |    False     ||           |  [-3.   2.   1.5]  |                  |   [-1.5]  |       |  [-3.   2.   1.5]  |   [ 1.   -1.   -1.  ] |              |              ||           |  [ 0.5 -0.5 -1.5]] |                  |   [-1. ]] |       |  [-0.5 -0.5 -2.5]] |   [ 0.75  0.25 -4.5 ] |              |              ||           |                    |                  |           |       |                    |   [ 1.    0.5  -3.5 ] |              |              ||           |                    |                  |           |       |                    |  [ 0.25 -4.   -0.25]] |              |              ||     22    | [[ 0.5  0.5  0. ]  |     [1 1 0]      |   [[ 1.]  |   1   | [[ 0.5  0.5  0. ]  |  [[ 0.5  -1.5  -3.  ] | [1 1 1 1 1]  |    False     ||           |  [-3.   2.   1.5]  |                  |    [-1.]  |       |  [-3.   2.   1.5]  |   [ 1.   -1.   -1.  ] |              |              ||           |  [-0.5 -0.5 -2.5]] |                  |   [-1.]]  |       |  [-0.5 -0.5 -2.5]] |   [ 0.75  0.25 -4.5 ] |              |              ||           |                    |                  |           |       |                    |   [ 1.    0.5  -3.5 ] |              |              ||           |                    |                  |           |       |                    |  [ 0.25 -4.   -0.25]] |              |              ||     23    | [[ 0.5  0.5  0. ]  |  [1.  0.5 1.5]   |  [[ 0.75] |   1   | [[-0.5  0.  -1.5]  |  [[-2.    1.   -3.  ] | [2 2 2 2 3]  |    False     ||           |  [-3.   2.   1.5]  |                  |   [ 0.25] |       |  [-2.   2.5  3. ]  |   [-0.5   0.5  -1.  ] |              |              ||           |  [-0.5 -0.5 -2.5]] |                  |  [-4.5 ]] |       |  [-0.5 -0.5 -2.5]] |   [-2.75  3.75 -4.5 ] |              |              ||           |                    |                  |           |       |                    |   [-2.    3.5  -3.5 ] |              |              ||           |                    |                  |           |       |                    |  [-0.5  -3.25 -0.25]] |              |              ||     24    | [[-0.5  0.  -1.5]  |     [1 1 1]      |  [[-2. ]  |   2   | [[-0.5  0.  -1.5]  |  [[-2.    1.   -3.  ] | [2 2 2 2 3]  |    False     ||           |  [-2.   2.5  3. ]  |                  |   [ 3.5]  |       |  [-2.   2.5  3. ]  |   [-0.5   0.5  -1.  ] |              |              ||           |  [-0.5 -0.5 -2.5]] |                  |   [-3.5]] |       |  [-0.5 -0.5 -2.5]] |   [-2.75  3.75 -4.5 ] |              |              ||           |                    |                  |           |       |                    |   [-2.    3.5  -3.5 ] |              |              ||           |                    |                  |           |       |                    |  [-0.5  -3.25 -0.25]] |              |              ||     25    | [[-0.5  0.  -1.5]  | [ 1.  -0.5  0. ] |  [[-0.5 ] |   3   | [[-0.5  0.  -1.5]  |  [[-2.    1.   -3.  ] | [2 2 2 2 3]  |    False     ||           |  [-2.   2.5  3. ]  |                  |   [-3.25] |       |  [-2.   2.5  3. ]  |   [-0.5   0.5  -1.  ] |              |              ||           |  [-0.5 -0.5 -2.5]] |                  |  [-0.25]] |       |  [-0.5 -0.5 -2.5]] |   [-2.75  3.75 -4.5 ] |              |              ||           |                    |                  |           |       |                    |   [-2.    3.5  -3.5 ] |              |              ||           |                    |                  |           |       |                    |  [-0.5  -3.25 -0.25]] |              |              |+-----------+--------------------+------------------+-----------+-------+--------------------+-----------------------+--------------+--------------+Minimum Squared Error (MSE) ProceduresMinimum Squared Error (MSE) Procedures Python code# Typically Y is rectangular, so the solution is given by: a=Y† bb = np.dot(y, a)# Cost function# e2= ∥ Ya−b ∥^2, J s (a)= 1/2‖ Ya−b ‖^2e = (np.dot(y, a)-b)(np.dot(y, a)-b)Widrow-Hoff (or LMS) Learning AlgorithmSequential Widrow-Hoff Learning Algorithm Python code# Sequential Widrow-Hoff Learning Algorithmimport numpy as npfrom prettytable import PrettyTable# configuration variables# -----------------------------------------------------------# initial valuesa = [-1.5,5,-1]# learning raten = 0.2# iterations# iterations = 150epoch = 2# dataset# -----------------------------------------------------------X = [[0,0],[1,0],[2,1],[0,1],[1,2]]Y = [1,1,1,-1,-1]# margin vectorb = np.ones(len(X))*2# sequential widrow-hoff learning algorithm# -----------------------------------------------------------result = []for o in range(epoch):    for i in range(len(Y)):        a_prev = a        if Y[i] == 1:            y = np.hstack((Y[i],X[i]))        else:            y = np.hstack((Y[i],np.array(X[i])*(-1)))        # calculate ay        ay = np.dot(a, y)                # calculate update part, η(b−ay)*y        update = n * (b[i] - ay) * y        # add update part to a        a = np.add(a, update)        cur_result = []        # evaluate        for index in range(len(Y)):            y = np.hstack((1,X[index]))            cur_result.append(np.dot(a, y))        # check if converage        is_converage = True        for index in range(len(Y)):            if cur_result[index] * Y[index] &lt; 0:                is_converage = False        # append result        result.append((str(i + 1 + (len(Y) * o)), np.round(a_prev, 4), np.round(y, 4), np.round(ay, 4), np.round(a, 4),np.round(cur_result, 4),is_converage))# prettytable# -----------------------------------------------------------pt = PrettyTable(('iteration', 'a', 'y', 'ay', 'a_new','over all result','is converage'))for row in result: pt.add_row(row)print(pt)Output+-----------+---------------------------+---------+---------+---------------------------+-------------------------------------------+-------------+| iteration | a                         | y       | ay      | a_new                     |              over all result              | is converge |+-----------+---------------------------+---------+---------+---------------------------+-------------------------------------------+-------------+|     1     | [-1.5  5.  -1. ]          | [1 1 2] | -1.5    | [-0.8  5.  -1. ]          |         [-0.8  4.2  8.2 -1.8  2.2]        |    False    ||     2     | [-0.8  5.  -1. ]          | [1 1 2] | 4.2     | [-1.24  4.56 -1.  ]       |      [-1.24  3.32  6.88 -2.24  1.32]      |    False    ||     3     | [-1.24  4.56 -1.  ]       | [1 1 2] | 6.88    | [-2.216  2.608 -1.976]    |    [-2.216  0.392  1.024 -4.192 -3.56 ]   |    False    ||     4     | [-2.216  2.608 -1.976]    | [1 1 2] | 4.192   | [-1.7776  2.608  -1.5376] | [-1.7776  0.8304  1.9008 -3.3152 -2.2448] |    False    ||     5     | [-1.7776  2.608  -1.5376] | [1 1 2] | 2.2448  | [-1.7286  2.657  -1.4397] | [-1.7286  0.9283  2.1456 -3.1683 -1.951 ] |    False    ||     6     | [-1.7286  2.657  -1.4397] | [1 1 2] | -1.7286 | [-0.9829  2.657  -1.4397] | [-0.9829  1.674   2.8913 -2.4226 -1.2053] |    False    ||     7     | [-0.9829  2.657  -1.4397] | [1 1 2] | 1.674   | [-0.9177  2.7222 -1.4397] | [-0.9177  1.8044  3.0869 -2.3574 -1.0749] |    False    ||     8     | [-0.9177  2.7222 -1.4397] | [1 1 2] | 3.0869  | [-1.1351  2.2874 -1.6571] | [-1.1351  1.1523  1.7826 -2.7922 -2.1618] |    False    ||     9     | [-1.1351  2.2874 -1.6571] | [1 1 2] | 2.7922  | [-0.9767  2.2874 -1.4986] | [-0.9767  1.3107  2.0995 -2.4753 -1.6865] |    False    ||     10    | [-0.9767  2.2874 -1.4986] | [1 1 2] | 1.6865  | [-1.0394  2.2247 -1.624 ] | [-1.0394  1.1853  1.786  -2.6634 -2.0627] |    False    |+-----------+---------------------------+---------+---------+---------------------------+-------------------------------------------+-------------+"
  },
  
  {
    "title": "Introduction to Deep Learning",
    "url": "/posts/Introduction-to-Deep-Learning/",
    "categories": "Deep Learning, Introduction",
    "tags": "Deep Learning, Machine Learning",
    "date": "2022-01-21 07:32:00 +0800",
    





    "snippet": "Improving PerformanceTo improve performance, we might  add more features  collect more data  use a model with more parameters (to define a more complex decision boundary)These choices interact:  Increasing the number of features → need more data to avoid overfitting  known as the “curse of dimensionality”  Increasing the number of parameters → need more data to avoid overfitting  known as “bias/variance trade-off”The Classifier Design CycleData collectionDesign Considerations:  What data to collect?What data is it possible to obtain? What data will be useful?  How much data to collect?– need representative set of examples for training and testing the classifierData Sources  One or more transducers (e.g. camera, microphone, etc.) extract information from the physical world.  Data my also come from other sources (e.g. databases, webpages, surveys, documents, etc.).Data Cleansing  Data may need to be preprocessed or cleaned (e.g. segmentation, noise reduction, outlier removal, dealing with missing values).  This provides the “input data”.Then input dataThis module will:  Assume that we have the input data  Concentrate on the subsequent stagesData is classified as:  discrete (integer/symbolic valued) or continuous (real/continuous valued).  univariate (containing one variable) or multivariate (containing multiple variable).Feature SelectionDesign Considerations:  What features are useful for classification? Knowledge of the task may help decideSelect features of the input data:  A feature can be any aspect, quality, or characteristic of the data.  Features may be discrete (e.g., labels such as “blue”, ”large”) or continuous (e.g., numeric values representing height, lightness).  Any number of features, d, can be used. The selected features form a “feature vector”Each datapoint/exemplar/sample is represented by the chosen set of features.  The combination of d features is a d-dimensional column vector called a feature vector.  The d-dimensional space defined by the feature vector is called the feature space.  a collection of datapoints/feature vectors is called a dataset.  datapoints can be represented as points in feature space; the result is a scatter plot.Model SelectionA model is the method used to perform the classification Design Considerations:What sort of model should be used?  e.g. Neural Network, SVM, Random Forest, etc.Different models will give different results  However, the only way to tell which model will work best is to try them.Parameter TuningThe model has parameters which need to be defined Design Considerations:● How to use the data to define the parameters?– There are many different algorithms for training classifiers (e.g. gradient descent, genetic algorithms)● What parameters to use for the training algorithm?– These are the hyper-parametersNeed to find parameter values that give best performance (e.g. minimum classification error) ● For more complex tasks exhaustive search becomes impracticalEvaluationDesign Considerations:  What is criteria for success?– There a many different metrics that can be used  e.g. error rate, cost, f1 -score, etc.– We use “performance” as a general term  Evaluation results my vary depending on how the dataset is split.  A better estimate of performance can be obtained by using multiple (k) splits and averaging results (called “k-fold cross-validation”):Simplest metric to measure performance is error-rate: The same metric can be expressed as:● % error (=100xerror-rate) (0 is optimal), or● accuracy (=1-error-rate) (1 is optimal), or● % accuracy or % correct (=100-%error) (100 is optimal)To explore where the prediction errors come from, we can create a confusion matrix, e.g.:If there are only two classes, then classifier is called a dichotomizer.error rate would improve if classifier always predicted “no car” (i.e. if classifier ignored the data!)Ignoring the data (by always predicting “false”) leads to poor performance measured with these metricsCalculating TP, FP, TN, FN Python codey =  [1,1,0,1,0,1,1]y_pred = [1,0,1,1,0,1,0]TP = 0TN = 0FP = 0FN = 0for index in range(len(y)):    if y_pred[index] == 1:        if y[index] == 1:            TP += 1        else:            FP +=1    if y_pred[index] == 0:        if y[index] == 0:            TN += 1        else:            FN += 1print('TP:',TP,'FP',FP,'TN',TN,'FN',FN)OutputTP: 3 FP 1 TN 1 FN 2Calculating error-rate, accuracy, recall, precision, f1 score Python codefrom prettytable import PrettyTablefrom fractions import Fractionerror_rate = Fraction(FP+FN,TP+TN+FP+FN)accuracy = Fraction(TP+TN,TP+TN+FP+FN)recall = Fraction(TP,TP+FN)precision = Fraction(TP,TP+FP)f1_score = Fraction(2*TP,2*TP+FP+FN)result = PrettyTable()result.field_names = ['error-rate','accuracy','recall','precision','f1_score']result.add_row([error_rate,accuracy,recall,precision,f1_score])print(result)Output+------------+----------+--------+-----------+----------+| error-rate | accuracy | recall | precision | f1_score |+------------+----------+--------+-----------+----------+|    3/7     |   4/7    |  3/5   |    3/4    |   2/3    |+------------+----------+--------+-----------+----------+LearningLearning is acquiring and improving performance through experience.Pretty much all animals with a central nervous system are capable of learning (even the simplest ones).We want computers to learn when it is too difficult or too expensive to program them directly to perform a task.Get the computer to program itself by showing examples of inputs and outputs.We will write a “parameterized” program, and let the learning algorithm find the set of parameters that best approximates the desired function or behaviour.Supervised learningRegression● Outputs are continuous variables (real numbers).● Also known as “curve fitting” or “function approximation”.● Allows us to perform interpolation.Classification● Outputs are discrete variables (category labels).● Aim is to assign feature vectors to classes.● e.g. learn a decision boundary that separates one class from the other.Unsupervised learningClustering  Discover “clumps” or “natural groupings” of points in the feature space  i.e. find groups of exemplars that are similar to each otherEmbedding  Discover a low-dimensional manifold or surface near which the data lives.Factorisation  Discover a set of components that can be combined together to reconstruct the samples in the data.Semi-supervised learningThe training data consists of both labelled and unlabelled exemplars.i.e. for some samples we have {x, ω} and for all other samples we have {x}.Particularly useful when the cost of having a human label data is expensive.Needs specialist learning algorithm that is able to update parameters both when a label is and isn’t provided.Reinforcement learningFor each input, the learning algorithm makes a decision: ω*=g(x)A critic, or teacher, compares ω* to the correct answer, and (occasionally) tells the learner if it was correct (reward) or incorrect (punishment).Learning uses reward and punishment to modify g to increase reward received for future decisions.instead of a dataset containing {x, ω} pairs, we get {x, g(x), reward/punishment}.Transfer learningTrain the classifier (with initially random parameters) on one task for which data is abundant.  i.e. use a dataset containing many {x, ω} pairs.Train the classifier (with initial parameters pre-trained on 1 st task) on second tasks for which data is less abundant.  i.e. use a dataset containing {x, ω} pairs.Expectation is that pre-training the classifier on the first task will help if perform well on the second task.  1st task needs to be “similar” to 2nd task.Hyper-parametersThe learning algorithm will also have parameters:  More practical algorithms may have many parameters.  These are called hyper-parameters (to distinguish them from the parameters being learnt by the classifier).  The choice of hyper-parameters may have a large influence on the performance of the trained classifier.  Choosing hyper-parameters:          exhaustive search (called grid search)      learning (called meta-learning)      Deployment"
  }
  
]

